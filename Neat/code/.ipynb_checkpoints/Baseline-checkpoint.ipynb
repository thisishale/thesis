{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries.\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "tf.enable_eager_execution()\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Activation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import h5py\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import librosa\n",
    "from librosa.core import stft, istft\n",
    "import smtplib\n",
    "from natsort import natsorted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load rbm weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pre_rbm = 'pre_44'\n",
    "Data_path = 'D:/studies/university/thesis/speech_separation_codes/du16/donesomestuff'\n",
    "# Data_path = os.getcwd()[0:-5]\n",
    "\n",
    "result_path = os.path.normpath(os.path.join(Data_path,'results','models'))\n",
    "l10=np.loadtxt(os.path.join(result_path,pre_rbm,'layer10.txt'))\n",
    "l11=np.loadtxt(os.path.join(result_path,pre_rbm,'layer11.txt'))\n",
    "l12=np.loadtxt(os.path.join(result_path,pre_rbm,'layer12.txt'))\n",
    "l20=np.loadtxt(os.path.join(result_path,pre_rbm,'layer20.txt'))\n",
    "l21=np.loadtxt(os.path.join(result_path,pre_rbm,'layer21.txt'))\n",
    "l22=np.loadtxt(os.path.join(result_path,pre_rbm,'layer22.txt'))\n",
    "l30=np.loadtxt(os.path.join(result_path,pre_rbm,'layer30.txt'))\n",
    "l31=np.loadtxt(os.path.join(result_path,pre_rbm,'layer31.txt'))\n",
    "l32=np.loadtxt(os.path.join(result_path,pre_rbm,'layer32.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define variables and filenames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_path = os.getcwd()[0:-5]\n",
    "tfrecord_folder_parent = 'tfrecord_files'\n",
    "tfrecord_folder = 'ftr_refrmd_30h_2_w7_norm'\n",
    "tfrecord_val_folder = 'ftr_refrmd_dev_w7_norm_15h'\n",
    "global ckpt_folder\n",
    "ckpt_folder = '208'\n",
    "global batch_size\n",
    "batch_size = 128\n",
    "len_data = (3337525, 257)\n",
    "val_len = (700000,1799)\n",
    "steps = len_data[0] // batch_size\n",
    "val_steps = val_len[0] // batch_size\n",
    "w=7\n",
    "buffersize = 1800000\n",
    "global datalen\n",
    "epochs_num = 30\n",
    "h = [2048,2048,1024]\n",
    "seed = 7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model and define it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 540
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 18928,
     "status": "error",
     "timestamp": 1565210785956,
     "user": {
      "displayName": "Hale Damirchi",
      "photoUrl": "https://lh5.googleusercontent.com/-w2toi7wPDL8/AAAAAAAAAAI/AAAAAAAAAUw/U8_SysYvTY4/s64/photo.jpg",
      "userId": "00567801216308932433"
     },
     "user_tz": -270
    },
    "id": "nZJhAXcklRLU",
    "outputId": "5d9aa0cc-a3ea-4649-ac34-a226553f4f56",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Data_path = os.getcwd()[0:-5]\n",
    "result_path = os.path.normpath(os.path.join(Data_path,'results'))\n",
    "\n",
    "seed = 7\n",
    "from tensorflow.keras.layers import Activation\n",
    "np.random.seed(seed)\n",
    "act = layers.LeakyReLU(alpha=0.1)\n",
    "model = Sequential()\n",
    "model.add(Dense(h[0], input_dim = w*len_data[1], kernel_initializer= tf.constant_initializer(l10), bias_initializer = tf.constant_initializer(l12)))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(h[1], kernel_initializer= tf.constant_initializer(l20), bias_initializer = tf.constant_initializer(l22)))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(h[2], kernel_initializer= tf.constant_initializer(l30), bias_initializer = tf.constant_initializer(l32)))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(len_data[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### this section is used for tfrecord files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _parse_function(example_proto):\n",
    "    features = {\"X\": tf.FixedLenFeature((w*257), tf.float32),\n",
    "              \"Y\": tf.FixedLenFeature((257), tf.float32)}\n",
    "    parsed_features = tf.parse_single_example(example_proto, features)\n",
    "    return parsed_features[\"X\"], parsed_features[\"Y\"]\n",
    "\n",
    "tfrecord_path = os.path.normpath(os.path.join(Data_path,tfrecord_folder_parent,tfrecord_folder))\n",
    "subset = 'filenum'\n",
    "files = tf.io.matching_files(os.path.normpath(os.path.join(tfrecord_path, '%s*' % subset)))\n",
    "shards = tf.data.Dataset.from_tensor_slices(files)\n",
    "shards = shards.shuffle(tf.cast(tf.shape(files)[0], tf.int64))\n",
    "dataset = shards.interleave(tf.data.TFRecordDataset, cycle_length=6)\n",
    "dataset = dataset.shuffle(buffer_size=buffersize)\n",
    "dataset = dataset.repeat(epochs_num)\n",
    "dataset = dataset.map(_parse_function)\n",
    "dataset = dataset.batch(batch_size)\n",
    "dataset = dataset.prefetch(batch_size)\n",
    "\n",
    "tfrecord_path_val = os.path.normpath(os.path.join(Data_path,tfrecord_folder_parent,tfrecord_val_folder))\n",
    "subset = 'filenum'\n",
    "files_val = tf.io.matching_files(os.path.normpath(os.path.join(tfrecord_path_val, '%s*' % subset)))\n",
    "shards_val = tf.data.Dataset.from_tensor_slices(files_val)\n",
    "dataset_val = tf.data.TFRecordDataset(shards_val)\n",
    "dataset_val = dataset_val.repeat(epochs_num)\n",
    "dataset_val = dataset_val.map(_parse_function)\n",
    "dataset_val = dataset_val.batch(batch_size)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define how we want the models to be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(os.path.join(result_path,\"checkpoints\",ckpt_folder,'*'))\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "if not os.path.exists(os.path.join(result_path,\"checkpoints\",ckpt_folder)):\n",
    "    os.makedirs(os.path.join(result_path,\"checkpoints\",ckpt_folder))\n",
    "print(datetime.datetime.now())\n",
    "checkpoint_path = os.path.normpath(os.path.join(result_path,\"checkpoints\",ckpt_folder,\"weights.{epoch:02d}.hdf5\"))\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path, verbose=1, save_weights_only=True, save_best_only = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_scheduler(epoch, lr):\n",
    "    decay_rate = 0.9\n",
    "    if epoch>10:\n",
    "        return lr * decay_rate\n",
    "    return lr\n",
    "\n",
    "lr_callbacks = tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "opt = optimizers.SGD(lr=0.1, momentum=0,nesterov = False)\n",
    "model.compile(loss='mse', optimizer=opt)\n",
    "history = model.fit( dataset, steps_per_epoch=steps,epochs=epochs_num,callbacks = [cp_callback, lr_callbacks], verbose=1,validation_data=dataset_val,validation_steps=val_steps)\n",
    "model_json = model.to_json()\n",
    "with open(os.path.normpath(os.path.join(result_path, 'models', \"model_\"+ckpt_folder+\".json\")), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(os.path.normpath(os.path.join(result_path, 'models', \"model_\"+ckpt_folder+\".h5\")))\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.normpath(os.path.join(result_path, 'models', \"model_\"+ckpt_folder+\"_whole.h5\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(os.path.normpath(os.path.join(result_path, 'models', \"model_\"+ckpt_folder+\".json\")), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(os.path.normpath(os.path.join(result_path, 'models', \"model_\"+ckpt_folder+\".h5\")))\n",
    "print(\"Saved model to disk\")\n",
    "# h5f.close()   \n",
    "print(datetime.datetime.now())\n",
    "%matplotlib inline\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'],label='train')\n",
    "plt.plot(history.history['val_loss'],label='validation')\n",
    "plt.legend()\n",
    "# plt.legend(['valid'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "\n",
    "\n",
    "plt.savefig(os.path.normpath(os.path.join(result_path,'images',ckpt_folder+'.png')))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "du16_2-autoencoder_pc.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "'Python Interactive'",
   "language": "python",
   "name": "d23b60bd-fde3-4bf7-9fcd-4972ac720241"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
