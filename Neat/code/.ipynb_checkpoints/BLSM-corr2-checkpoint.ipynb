{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AoW-DJHJN739"
   },
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2020,
     "status": "ok",
     "timestamp": 1572770690617,
     "user": {
      "displayName": "Hale Damirchi",
      "photoUrl": "https://lh5.googleusercontent.com/-w2toi7wPDL8/AAAAAAAAAAI/AAAAAAAAAUw/U8_SysYvTY4/s64/photo.jpg",
      "userId": "00567801216308932433"
     },
     "user_tz": -210
    },
    "id": "1fpLNXcNlRLL",
    "outputId": "d118d3ff-a18a-49aa-d113-e5bce1fc2b6d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import libraries.\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "# tf.enable_eager_execution()\n",
    "from tensorflow.keras.layers import Activation, multiply\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Reshape, TimeDistributed, Bidirectional\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import ConvLSTM2D\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "import h5py\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "# import librosa\n",
    "# from librosa.core import stft, istft\n",
    "# from natsort import natsorted\n",
    "print('imported')\n",
    "# #######################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define variables\n",
    "define model name, number of epochs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1042,
     "status": "ok",
     "timestamp": 1572770696598,
     "user": {
      "displayName": "Hale Damirchi",
      "photoUrl": "https://lh5.googleusercontent.com/-w2toi7wPDL8/AAAAAAAAAAI/AAAAAAAAAUw/U8_SysYvTY4/s64/photo.jpg",
      "userId": "00567801216308932433"
     },
     "user_tz": -210
    },
    "id": "K0kyLLwfqmZJ",
    "outputId": "a6079220-b1dc-4601-f0e7-9f96a79746de"
   },
   "outputs": [],
   "source": [
    "\n",
    "Data_path = os.getcwd()[0:-5]\n",
    "global ckpt_folder\n",
    "ckpt_folder = '223'\n",
    "global batch_size\n",
    "timestep = 16\n",
    "# len_data = (3337525, 257)\n",
    "# val_len = (700000,257)\n",
    "batch_size = 32\n",
    "w=1\n",
    "I=0\n",
    "# global datalen\n",
    "# datalen=len_data[0]\n",
    "seed = 7\n",
    "epochs_num = 30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_path = os.path.normpath(os.path.join(Data_path,'h5_files'))\n",
    "input_name = 'mixed_log_30h_norm'\n",
    "input_path = os.path.normpath(os.path.join(write_path,input_name))\n",
    "target_name = 'single_dataset_log_30h'\n",
    "target_path = os.path.normpath(os.path.join(write_path,target_name))\n",
    "inp = h5py.File(input_path+'.hdf5','r')\n",
    "tar = h5py.File(target_path+'.hdf5', 'r')\n",
    "X=inp[input_name][:]\n",
    "Y=tar[target_name][:]\n",
    "inp.close()\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### deine length of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[0:(len(X)//timestep)*timestep].reshape((-1,timestep,257))\n",
    "Y = Y[0:(len(Y)//timestep)*timestep].reshape((-1,timestep,257))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[0:(len(X)//batch_size)*batch_size]\n",
    "Y = Y[0:(len(Y)//batch_size)*batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### do the same for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_path = os.path.normpath(os.path.join(Data_path,'h5_files'))\n",
    "input_name_val = 'mixed_log_dev_norm_30h'#dev3\n",
    "input_path_val = os.path.normpath(os.path.join(write_path,input_name_val))\n",
    "target_name_val = 'single_dataset_log_dev'\n",
    "target_path_val = os.path.normpath(os.path.join(write_path,target_name_val))\n",
    "inp_val = h5py.File(input_path_val+'.hdf5','r')\n",
    "tar_val = h5py.File(target_path_val+'.hdf5', 'r')\n",
    "X_val = inp_val[input_name_val][:]\n",
    "Y_val = tar_val[target_name_val][:]\n",
    "inp_val.close()\n",
    "tar_val.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_val[0:(len(X_val)//timestep)*timestep].reshape((-1,timestep,257))\n",
    "Y_val = Y_val[0:(len(Y_val)//timestep)*timestep].reshape((-1,timestep,257))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_val[0:(len(X_val)//batch_size)*batch_size]\n",
    "Y_val = Y_val[0:(len(Y_val)//batch_size)*batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MrEefMiMqmZW"
   },
   "outputs": [],
   "source": [
    "result_path = os.path.normpath(os.path.join(Data_path,'results'))\n",
    "seed = 7\n",
    "inputs = Input(shape=(timestep,w*257))\n",
    "act1 = layers.LeakyReLU(alpha=0.1)\n",
    "act2 = layers.LeakyReLU(alpha=0.1)\n",
    "act3 = layers.LeakyReLU(alpha=0.1)\n",
    "output_rnn =Bidirectional(LSTM(257,\n",
    "          return_sequences=True,\n",
    "          stateful=False))(inputs)\n",
    "output_rnn =Bidirectional(LSTM(257,\n",
    "          return_sequences=True,\n",
    "          stateful=False))(output_rnn)\n",
    "\n",
    "\n",
    "output_dense = TimeDistributed(Dense(257))(output_rnn)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create loss function\n",
    "this function makes use of the mutual information in optimizing the network. formulas are described in the thesis file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    def mel_filter(dft_signal):\n",
    "        sample_rate = 16000\n",
    "        nfilt = 40\n",
    "        NFFT = 512\n",
    "        low_freq_mel = 0\n",
    "        high_freq_mel = (2595 * np.log10(1 + (sample_rate / 2) / 700))  # Convert Hz to Mel\n",
    "        mel_points = np.linspace(low_freq_mel, high_freq_mel, nfilt + 2)  # Equally spaced in Mel scale\n",
    "        hz_points = (700 * (10**(mel_points / 2595) - 1))  # Convert Mel to Hz\n",
    "        binf = (np.floor((NFFT + 1) * hz_points / sample_rate)).astype(np.int32)\n",
    "        \n",
    "        fbank = np.zeros((nfilt, 257),dtype=np.float32)\n",
    "        for m in range(1, 41):\n",
    "            f_m_minus = np.int((binf[m - 1]+binf[m])/2)   # left\n",
    "            f_m = np.int(binf[m])           # center\n",
    "            f_m_plus = np.int((binf[m + 1]+binf[m])/2)     # right\n",
    "\n",
    "            for k in range(f_m_minus, f_m):\n",
    "                fbank[m - 1, k] = 1\n",
    "            for k in range(f_m, f_m_plus):\n",
    "                fbank[m - 1, k] = 1\n",
    "                \n",
    "        fbank=fbank.reshape((1,fbank.shape[0],fbank.shape[1]))\n",
    "        fbank = np.repeat(fbank, batch_size, axis=0)\n",
    "        filter_banks1 = tf.einsum('mnp,mqp->mnq', dft_signal, tf.convert_to_tensor(fbank))\n",
    "        filter_banks = tf.where(tf.equal(filter_banks1,0), 1e-7+tf.zeros(tf.shape(tf.equal(filter_banks1,0))),\n",
    "                                filter_banks1)\n",
    "\n",
    "        return filter_banks\n",
    "    true = tf.sqrt(tf.pow(tf.constant(10, dtype=tf.float32),y_true))\n",
    "    pred = tf.sqrt(tf.pow(tf.constant(10, dtype=tf.float32),y_pred))\n",
    "    filtered_true = mel_filter(true)\n",
    "    filtered_pred = mel_filter(pred)\n",
    "    c=0\n",
    "\n",
    "    aa = filtered_true-tf.expand_dims(tf.reduce_mean(filtered_true,axis=1),axis=1)\n",
    "    bb = filtered_pred-tf.expand_dims(tf.reduce_mean(filtered_pred,axis=1),axis=1)\n",
    "    cov = tf.reduce_mean(aa*bb,axis=1)\n",
    "\n",
    "    print(cov.shape)\n",
    "    std1 = tf.math.reduce_std(filtered_true,axis=1)\n",
    "    std2 = tf.math.reduce_std(filtered_pred,axis=1)\n",
    "    std1 = tf.where(tf.equal(std1,0), 1e-7+tf.zeros(tf.shape(tf.equal(std1,0))),\n",
    "                        std1)\n",
    "    std2 = tf.where(tf.equal(std2,0), 1e-7+tf.zeros(tf.shape(tf.equal(std2,0))),\n",
    "                        std2)\n",
    "    ro = cov/(std1*std2)\n",
    "    return (0.8*ro)+(0.2*tf.reduce_mean(tf.square(y_true-y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_scheduler(epoch, lr):\n",
    "    decay_rate = 0.9\n",
    "    if epoch>10:\n",
    "        return lr * decay_rate\n",
    "    return lr\n",
    "opt = optimizers.Adam(lr=0.0005)\n",
    "lr_callbacks = tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19422,
     "status": "error",
     "timestamp": 1571261249813,
     "user": {
      "displayName": "Hale Damirchi",
      "photoUrl": "https://lh5.googleusercontent.com/-w2toi7wPDL8/AAAAAAAAAAI/AAAAAAAAAUw/U8_SysYvTY4/s64/photo.jpg",
      "userId": "00567801216308932433"
     },
     "user_tz": -210
    },
    "id": "Oq7TujE7efoX",
    "outputId": "81be08a2-2338-435a-92e8-0480aa2b6b12",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files = glob.glob(os.path.join(result_path,\"checkpoints\",ckpt_folder,'*'))\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "if not os.path.exists(os.path.join(result_path,\"checkpoints\",ckpt_folder)):\n",
    "    os.makedirs(os.path.join(result_path,\"checkpoints\",ckpt_folder))\n",
    "print(datetime.datetime.now())\n",
    "checkpoint_path = os.path.normpath(os.path.join(result_path,\"checkpoints\",ckpt_folder,\"weights.{epoch:02d}.hdf5\"))\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path, verbose=1, save_weights_only=True, save_best_only=True)\n",
    "opt = optimizers.Adam(lr=0.0001)\n",
    "model.compile(loss=custom_loss, optimizer=opt)\n",
    "history = model.fit( X, Y, batch_size = batch_size, epochs = epochs_num, callbacks = [cp_callback,TrainValTensorBoard(write_graph=False)], verbose=1,validation_data = (X_val,Y_val))\n",
    "# model.fit_generator(X,Y,batch_size = batch_size, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1426,
     "status": "ok",
     "timestamp": 1570653186035,
     "user": {
      "displayName": "Hale Damirchi",
      "photoUrl": "https://lh5.googleusercontent.com/-w2toi7wPDL8/AAAAAAAAAAI/AAAAAAAAAUw/U8_SysYvTY4/s64/photo.jpg",
      "userId": "00567801216308932433"
     },
     "user_tz": -210
    },
    "id": "YLSe2YSpqmZa",
    "outputId": "0a9fd73d-afcf-45de-e1ce-c3e1847153ed"
   },
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(os.path.normpath(os.path.join(result_path, 'models', \"model_\"+ckpt_folder+\".json\")), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(os.path.normpath(os.path.join(result_path, 'models', \"model_\"+ckpt_folder+\".h5\")))\n",
    "print(\"Saved model to disk\")\n",
    "# h5f.close()   \n",
    "print(datetime.datetime.now())\n",
    "%matplotlib inline\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'],label='train')\n",
    "plt.plot(history.history['val_loss'],label='validation')\n",
    "plt.legend()\n",
    "# plt.legend(['valid'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "\n",
    "\n",
    "plt.savefig(os.path.normpath(os.path.join(result_path,'images',ckpt_folder+'.png')))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.normpath(os.path.join(result_path, 'models', \"model_\"+ckpt_folder+\"_whole.h5\")))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "du16_2_rnn_mi.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "'Python Interactive'",
   "language": "python",
   "name": "d23b60bd-fde3-4bf7-9fcd-4972ac720241"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
