{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AoW-DJHJN739"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 23091,
     "status": "ok",
     "timestamp": 1572770481836,
     "user": {
      "displayName": "Hale Damirchi",
      "photoUrl": "https://lh5.googleusercontent.com/-w2toi7wPDL8/AAAAAAAAAAI/AAAAAAAAAUw/U8_SysYvTY4/s64/photo.jpg",
      "userId": "00567801216308932433"
     },
     "user_tz": -210
    },
    "id": "5mvAxmdEoRVv",
    "outputId": "8584d33f-e7b7-4c73-da54-d5d39300b2fe"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1235,
     "status": "ok",
     "timestamp": 1571262246894,
     "user": {
      "displayName": "Hale Damirchi",
      "photoUrl": "https://lh5.googleusercontent.com/-w2toi7wPDL8/AAAAAAAAAAI/AAAAAAAAAUw/U8_SysYvTY4/s64/photo.jpg",
      "userId": "00567801216308932433"
     },
     "user_tz": -210
    },
    "id": "Y68B1YxPhxCG",
    "outputId": "a89a35ef-54b1-4f9b-cf47-877e899caefa"
   },
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "cuda.select_device(0)\n",
    "cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2020,
     "status": "ok",
     "timestamp": 1572770690617,
     "user": {
      "displayName": "Hale Damirchi",
      "photoUrl": "https://lh5.googleusercontent.com/-w2toi7wPDL8/AAAAAAAAAAI/AAAAAAAAAUw/U8_SysYvTY4/s64/photo.jpg",
      "userId": "00567801216308932433"
     },
     "user_tz": -210
    },
    "id": "1fpLNXcNlRLL",
    "outputId": "d118d3ff-a18a-49aa-d113-e5bce1fc2b6d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import libraries.\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "# tf.enable_eager_execution()\n",
    "from tensorflow.keras.layers import Activation, multiply\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Reshape, TimeDistributed, Bidirectional\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import ConvLSTM2D\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "import h5py\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "# import librosa\n",
    "# from librosa.core import stft, istft\n",
    "# from natsort import natsorted\n",
    "print('imported')\n",
    "# #######################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1042,
     "status": "ok",
     "timestamp": 1572770696598,
     "user": {
      "displayName": "Hale Damirchi",
      "photoUrl": "https://lh5.googleusercontent.com/-w2toi7wPDL8/AAAAAAAAAAI/AAAAAAAAAUw/U8_SysYvTY4/s64/photo.jpg",
      "userId": "00567801216308932433"
     },
     "user_tz": -210
    },
    "id": "K0kyLLwfqmZJ",
    "outputId": "a6079220-b1dc-4601-f0e7-9f96a79746de"
   },
   "outputs": [],
   "source": [
    "# Data_path = 'D:/studies/university/thesis/speech_separation_codes/du16/donesomestuff'\n",
    "# Data_path = 'D:/hale'\n",
    "# Data_path = '/content/drive/My Drive/thesis/datasets'\n",
    "Data_path = os.getcwd()[0:-5]\n",
    "pretrained_name = '184'\n",
    "global ckpt_folder\n",
    "ckpt_folder = '184'\n",
    "global batch_size\n",
    "timestep = 16\n",
    "# len_data = (3337525, 257)\n",
    "# val_len = (700000,257)\n",
    "batch_size = 32\n",
    "w=1\n",
    "I=0\n",
    "# global datalen\n",
    "# datalen=len_data[0]\n",
    "seed = 7\n",
    "epochs_num = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_path = os.path.normpath(os.path.join(Data_path,'h5_files'))\n",
    "input_name = 'mixed_log_30h_norm'\n",
    "input_path = os.path.normpath(os.path.join(write_path,input_name))\n",
    "target_name = 'single_dataset_log_30h'\n",
    "target_path = os.path.normpath(os.path.join(write_path,target_name))\n",
    "inp = h5py.File(input_path+'.hdf5','r')\n",
    "tar = h5py.File(target_path+'.hdf5', 'r')\n",
    "X=inp[input_name][:]\n",
    "Y=tar[target_name][:]\n",
    "inp.close()\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[0:(len(X)//timestep)*timestep].reshape((-1,timestep,257))\n",
    "Y = Y[0:(len(Y)//timestep)*timestep].reshape((-1,timestep,257))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_path = os.path.normpath(os.path.join(Data_path,'h5_files'))\n",
    "input_name_val = 'mixed_log_dev_norm_30h'#dev3\n",
    "input_path_val = os.path.normpath(os.path.join(write_path,input_name_val))\n",
    "target_name_val = 'single_dataset_log_dev'\n",
    "target_path_val = os.path.normpath(os.path.join(write_path,target_name_val))\n",
    "inp_val = h5py.File(input_path_val+'.hdf5','r')\n",
    "tar_val = h5py.File(target_path_val+'.hdf5', 'r')\n",
    "X_val = inp_val[input_name_val][:]\n",
    "Y_val = tar_val[target_name_val][:]\n",
    "inp_val.close()\n",
    "tar_val.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_val[0:(len(X_val)//timestep)*timestep].reshape((-1,timestep,257))\n",
    "Y_val = Y_val[0:(len(Y_val)//timestep)*timestep].reshape((-1,timestep,257))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MrEefMiMqmZW"
   },
   "outputs": [],
   "source": [
    "# result_path = '/content/drive/My Drive/thesis/results'\n",
    "result_path = os.path.normpath(os.path.join(Data_path,'results'))\n",
    "seed = 7\n",
    "# np.random.seed(seed)\n",
    "inputs = Input(shape=(timestep,w*257))\n",
    "act1 = layers.LeakyReLU(alpha=0.1)\n",
    "act2 = layers.LeakyReLU(alpha=0.1)\n",
    "act3 = layers.LeakyReLU(alpha=0.1)\n",
    "# a layer instance is callable on a tensor, and returns a tensor\n",
    "output_rnn =Bidirectional(LSTM(257,\n",
    "          return_sequences=True,\n",
    "          stateful=False))(inputs)\n",
    "# output_rnn = LayerNormalization()(output_rnn)\n",
    "output_rnn =Bidirectional(LSTM(257,\n",
    "          return_sequences=True,\n",
    "          stateful=False))(output_rnn)\n",
    "# output_rnn = LayerNormalization()(output_rnn)\n",
    "# output_rnn =Bidirectional(LSTM(257,\n",
    "#           batch_input_shape=(batch_size,timestep,w*257),\n",
    "#           batch_size=batch_size,\n",
    "#           return_sequences=True,\n",
    "#           stateful=False))(output_rnn)\n",
    "# output_rnn = LayerNormalization()(output_rnn)\n",
    "\n",
    "output_dense = TimeDistributed(Dense(257))(output_rnn)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    def compute_delta(signal_true,signal_pred): \n",
    "        tau=3\n",
    "        sum_temp = 0\n",
    "        for i in range(tau-1,256-tau+2):#(2,254)\n",
    "            for j in range(1,tau):\n",
    "                temp_true = signal_true[:,:,i+j]-signal_true[:,:,i-j]\n",
    "                temp_pred = signal_pred[:,:,i+j]-signal_pred[:,:,i-j]\n",
    "                sum_temp = K.square(temp_true-temp_pred) + sum_temp\n",
    "        i=0\n",
    "        for j in range(1,tau):\n",
    "            temp_true = signal_true[:,:,i+j]-signal_true[:,:,i]\n",
    "            temp_pred = signal_pred[:,:,i+j]-signal_pred[:,:,i]\n",
    "            sum_temp = K.square(temp_true-temp_pred) + sum_temp\n",
    "        i=1\n",
    "        print('here2')\n",
    "        for j in range(1,tau):\n",
    "            temp_true = signal_true[:,:,i+j]-signal_true[:,:,i-1]\n",
    "            temp_pred = signal_pred[:,:,i+j]-signal_pred[:,:,i-1]\n",
    "            sum_temp = K.square(temp_true-temp_pred) + sum_temp\n",
    "        i=256\n",
    "        print('hre')\n",
    "        for j in range(1,tau):\n",
    "            temp_true = signal_true[:,:,i]-signal_true[:,:,i-j]\n",
    "            temp_pred = signal_pred[:,:,i]-signal_pred[:,:,i-j]\n",
    "            sum_temp = K.square(temp_true-temp_pred) + sum_temp\n",
    "        i=255\n",
    "        print('e')\n",
    "        for j in range(1,tau):\n",
    "            temp_true = signal_true[:,:,i+1]-signal_true[:,:,i-j]\n",
    "            temp_pred = signal_pred[:,:,i+1]-signal_pred[:,:,i-j]\n",
    "            sum_temp = K.square(temp_true-temp_pred) + sum_temp        \n",
    "        return sum_temp\n",
    "    print('hereout')\n",
    "    sum_delta = compute_delta(y_true,y_pred)\n",
    "    print(sum_delta)\n",
    "    loss = (2.3*sum_delta/257) + (0.1*K.mean(K.square(y_true-y_pred), axis=-1))\n",
    "    print(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.python.eager import context\n",
    "\n",
    "class TrainValTensorBoard(TensorBoard):\n",
    "    def __init__(self, log_dir=os.path.normpath(os.path.join(Data_path,'logs', ckpt_folder)), **kwargs):\n",
    "        self.val_log_dir = os.path.join(log_dir, 'validation')\n",
    "        training_log_dir = os.path.join(log_dir, 'training')\n",
    "        super(TrainValTensorBoard, self).__init__(training_log_dir, **kwargs)\n",
    "\n",
    "    def set_model(self, model):\n",
    "        if context.executing_eagerly():\n",
    "            self.val_writer = tf.contrib.summary.create_file_writer(self.val_log_dir)\n",
    "        else:\n",
    "            self.val_writer = tf.summary.FileWriter(self.val_log_dir)\n",
    "        super(TrainValTensorBoard, self).set_model(model)\n",
    "\n",
    "    def _write_custom_summaries(self, step, logs=None):\n",
    "        logs = logs or {}\n",
    "        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if 'val_' in k}\n",
    "        if context.executing_eagerly():\n",
    "            with self.val_writer.as_default(), tf.contrib.summary.always_record_summaries():\n",
    "                for name, value in val_logs.items():\n",
    "                    tf.contrib.summary.scalar(name, value.item(), step=step)\n",
    "        else:\n",
    "            for name, value in val_logs.items():\n",
    "                summary = tf.Summary()\n",
    "                summary_value = summary.value.add()\n",
    "                summary_value.simple_value = value.item()\n",
    "                summary_value.tag = name\n",
    "                self.val_writer.add_summary(summary, step)\n",
    "        self.val_writer.flush()\n",
    "\n",
    "        logs = {k: v for k, v in logs.items() if not 'val_' in k}\n",
    "        super(TrainValTensorBoard, self)._write_custom_summaries(step, logs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        super(TrainValTensorBoard, self).on_train_end(logs)\n",
    "        self.val_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_scheduler(epoch, lr):\n",
    "    decay_rate = 0.9\n",
    "    if epoch>10:\n",
    "        return lr * decay_rate\n",
    "    return lr\n",
    "opt = optimizers.Adam(lr=0.001)\n",
    "lr_callbacks = tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19422,
     "status": "error",
     "timestamp": 1571261249813,
     "user": {
      "displayName": "Hale Damirchi",
      "photoUrl": "https://lh5.googleusercontent.com/-w2toi7wPDL8/AAAAAAAAAAI/AAAAAAAAAUw/U8_SysYvTY4/s64/photo.jpg",
      "userId": "00567801216308932433"
     },
     "user_tz": -210
    },
    "id": "Oq7TujE7efoX",
    "outputId": "81be08a2-2338-435a-92e8-0480aa2b6b12",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files = glob.glob(os.path.join(result_path,\"checkpoints\",ckpt_folder,'*'))\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "if not os.path.exists(os.path.join(result_path,\"checkpoints\",ckpt_folder)):\n",
    "    os.makedirs(os.path.join(result_path,\"checkpoints\",ckpt_folder))\n",
    "print(datetime.datetime.now())\n",
    "checkpoint_path = os.path.normpath(os.path.join(result_path,\"checkpoints\",ckpt_folder,\"weights.{epoch:02d}.hdf5\"))\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path, verbose=1, save_weights_only=True, save_best_only=True)\n",
    "model.compile(loss=custom_loss, optimizer=opt)\n",
    "history = model.fit( X, Y, batch_size = batch_size, epochs = epochs_num, callbacks = [cp_callback,lr_callbacks, TrainValTensorBoard(write_graph=False)], verbose=1,validation_data = (X_val,Y_val))\n",
    "# model.fit_generator(X,Y,batch_size = batch_size, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1426,
     "status": "ok",
     "timestamp": 1570653186035,
     "user": {
      "displayName": "Hale Damirchi",
      "photoUrl": "https://lh5.googleusercontent.com/-w2toi7wPDL8/AAAAAAAAAAI/AAAAAAAAAUw/U8_SysYvTY4/s64/photo.jpg",
      "userId": "00567801216308932433"
     },
     "user_tz": -210
    },
    "id": "YLSe2YSpqmZa",
    "outputId": "0a9fd73d-afcf-45de-e1ce-c3e1847153ed"
   },
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(os.path.normpath(os.path.join(result_path, 'models', \"model_\"+ckpt_folder+\".json\")), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(os.path.normpath(os.path.join(result_path, 'models', \"model_\"+ckpt_folder+\".h5\")))\n",
    "print(\"Saved model to disk\")\n",
    "# h5f.close()   \n",
    "print(datetime.datetime.now())\n",
    "%matplotlib inline\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'],label='train')\n",
    "plt.plot(history.history['val_loss'],label='validation')\n",
    "plt.legend()\n",
    "# plt.legend(['valid'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "\n",
    "\n",
    "plt.savefig(os.path.normpath(os.path.join(result_path,'images',ckpt_folder+'.png')))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.normpath(os.path.join(result_path, 'models', \"model_\"+ckpt_folder+\"_whole.h5\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "awYHfPFcXKSo"
   },
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    def compute_delta(signal_true,signal_pred): \n",
    "        tau=3\n",
    "        sum_temp = tf.zeros((32,16))\n",
    "        for i in range(tau-1,256-tau+1):#(2,254)\n",
    "            for j in range(1,tau):\n",
    "                temp_true = tau*(signal_true[:,:,i+j]-signal_true[:,:,i-j])\n",
    "                temp_pred = tau*(signal_pred[:,:,i+j]-signal_pred[:,:,i-j])\n",
    "                sum_temp = temp_true-temp_pred + sum_temp\n",
    "        i=0\n",
    "        for j in range(1,tau):\n",
    "            temp_true = tau*(signal_true[:,:,i+j]-signal_true[:,:,i])\n",
    "            temp_pred = tau*(signal_pred[:,:,i+j]-signal_pred[:,:,i])\n",
    "            sum_temp = temp_true-temp_pred + sum_temp\n",
    "        i=1\n",
    "        print('here2')\n",
    "        for j in range(1,tau):\n",
    "            temp_true = tau*(signal_true[:,:,i+j]-signal_true[:,:,i-1])\n",
    "            temp_pred = tau*(signal_pred[:,:,i+j]-signal_pred[:,:,i-1])\n",
    "            sum_temp = temp_true-temp_pred + sum_temp\n",
    "        i=256\n",
    "        print('hre')\n",
    "        for j in range(1,tau):\n",
    "            temp_true = tau*(signal_true[:,:,i]-signal_true[:,:,i-j])\n",
    "            temp_pred = tau*(signal_pred[:,:,i]-signal_pred[:,:,i-j])\n",
    "            sum_temp = temp_true-temp_pred + sum_temp\n",
    "        i=255\n",
    "        print('e')\n",
    "        for j in range(1,tau):\n",
    "            temp_true = tau*(signal_true[:,:,i+1]-signal_true[:,:,i-j])\n",
    "            temp_pred = tau*(signal_pred[:,:,i+1]-signal_pred[:,:,i-j])\n",
    "            sum_temp = temp_true-temp_pred + sum_temp        \n",
    "        return sum_temp/10\n",
    "    print('hereout')\n",
    "    sum_delta = compute_delta(y_true,y_pred)\n",
    "    print(sum_delta)\n",
    "    loss = K.mean(K.square(sum_delta), axis=0)+K.mean(K.square(y_true-y_pred), axis=0)\n",
    "    print(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    def compute_delta(signal_true,signal_pred):\n",
    "        if i=0:\n",
    "            temp_true = signal_true[:,:,i]+signal_true[:,:,i]+signal_true[:,:,i]+signal_true[:,:,i+1]+signal_true[:,:,i+2]\n",
    "            temp_pred = signal_pred[:,:,i]+signal_pred[:,:,i]+signal_pred[:,:,i]+signal_pred[:,:,i+1]+signal_pred[:,:,i+2]\n",
    "            sum_temp = (2*temp_true)-(2*temp_pred)  \n",
    "        if i=1:\n",
    "            temp_true = signal_true[:,:,i-1]+signal_true[:,:,i-1]+signal_true[:,:,i]+signal_true[:,:,i+1]+signal_true[:,:,i+2]\n",
    "            temp_pred = signal_pred[:,:,i-1]+signal_pred[:,:,i-1]+signal_pred[:,:,i]+signal_pred[:,:,i+1]+signal_pred[:,:,i+2]\n",
    "            sum_temp = (2*temp_true)-(2*temp_pred)    \n",
    "        for i in range(2,255):\n",
    "            temp_true = signal_true[:,:,i-2]+signal_true[:,:,i-1]+signal_true[:,:,i]+signal_true[:,:,i+1]+signal_true[:,:,i+2]\n",
    "            temp_pred = signal_pred[:,:,i-2]+signal_pred[:,:,i-1]+signal_pred[:,:,i]+signal_pred[:,:,i+1]+signal_pred[:,:,i+2]\n",
    "            sum_temp = (2*temp_true)-(2*temp_pred)\n",
    "        if i=256:\n",
    "            temp_true = signal_true[:,:,i]+signal_true[:,:,i]+signal_true[:,:,i]+signal_true[:,:,i+1]+signal_true[:,:,i+1]\n",
    "            temp_pred = signal_pred[:,:,i]+signal_pred[:,:,i]+signal_pred[:,:,i]+signal_pred[:,:,i+1]+signal_pred[:,:,i+1]\n",
    "            sum_temp = (2*temp_true)-(2*temp_pred)  \n",
    "        if i=257:\n",
    "            temp_true = signal_true[:,:,i-1]+signal_true[:,:,i-1]+signal_true[:,:,i]+signal_true[:,:,i]+signal_true[:,:,i]\n",
    "            temp_pred = signal_pred[:,:,i-1]+signal_pred[:,:,i-1]+signal_pred[:,:,i]+signal_pred[:,:,i]+signal_pred[:,:,i]\n",
    "            sum_temp = (2*temp_true)-(2*temp_pred)  \n",
    "        return sum_temp/8\n",
    "    sum_delta = compute_delta(y_true,y_pred)\n",
    "    return K.mean(K.square(sum_delta), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "colab_type": "code",
    "id": "nZJhAXcklRLU",
    "outputId": "a1629919-4b93-44f6-f742-54d1d604a4ad"
   },
   "outputs": [],
   "source": [
    "# import libraries.\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "# tf.enable_eager_execution()\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Reshape\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import h5py\n",
    "import glob\n",
    "######################\n",
    "#import libraries.\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import librosa\n",
    "from librosa.core import stft, istft\n",
    "####import sounddevice as sd\n",
    "import time\n",
    "print('imported')\n",
    "\n",
    "result_path = '/content/drive/My Drive/thesis/results'\n",
    "# result_path = os.path.normpath(os.path.join(Data_path,'results'))\n",
    "\n",
    "seed = 7\n",
    "from tensorflow.keras.layers import Activation\n",
    "# from keras.layers import Activation\n",
    "np.random.seed(seed)\n",
    "\n",
    "inputs = Input(shape=(w*257,))\n",
    "inputs_reshaped = Reshape((-1, w*257), input_shape=(w*257,))(inputs)\n",
    "act1 = layers.LeakyReLU(alpha=0.1)\n",
    "# a layer instance is callable on a tensor, and returns a tensor\n",
    "output_rnn =LSTM(1024,\n",
    "          input_shape=(-1,w*257),\n",
    "          batch_size=batch_size,\n",
    "          stateful=False)(inputs_reshaped)\n",
    "output_dense = Dense(1024)(output_rnn)\n",
    "# output_dense = TimeDistributed(Dense(1024), input_shape=(32, 100))(output_rnn)\n",
    "output_dense = act1(output_dense)\n",
    "out = Dense(257)(output_dense)\n",
    "model = Model(inputs=inputs, outputs=out)\n",
    "# This creates a model that includes\n",
    "# the Input layer and three Dense layers\n",
    "\n",
    "#############################################\n",
    "import os\n",
    "\n",
    "def _parse_function1(example_proto):\n",
    "    features = {\"X\": tf.FixedLenFeature((w*257), tf.float32),\n",
    "              \"Y\": tf.FixedLenFeature((257), tf.float32)}\n",
    "    parsed_features = tf.parse_single_example(example_proto, features)\n",
    "    return parsed_features[\"X\"]\n",
    "\n",
    "def _parse_function2(example_proto):\n",
    "    features = {\"X\": tf.FixedLenFeature((w*257), tf.float32),\n",
    "              \"Y\": tf.FixedLenFeature((257), tf.float32)}\n",
    "    parsed_features = tf.parse_single_example(example_proto, features)\n",
    "    return parsed_features[\"Y\"]\n",
    "\n",
    "tfrecord_path = os.path.normpath(os.path.join(Data_path,tfrecord_folder_parent,tfrecord_folder))\n",
    "\n",
    "\n",
    "subset = 'filenum'\n",
    "files1 = tf.matching_files(os.path.normpath(os.path.join(tfrecord_path, '%s*' % subset)))\n",
    "shards1 = tf.data.Dataset.from_tensor_slices(files1)\n",
    "# shards = shards.shuffle(tf.cast(tf.shape(files)[0], tf.int64))\n",
    "shards1 = shards1.repeat()\n",
    "# dataset = shards.interleave(tf.data.TFRecordDataset, cycle_length=20)\n",
    "dataset1 = tf.data.TFRecordDataset(shards1)\n",
    "#it reads from 6 files simoltaneously\n",
    "# dataset = dataset.shuffle(buffer_size=buffersize)\n",
    "parser1 = _parse_function1\n",
    "dataset1 = dataset1.apply(\n",
    "   tf.data.experimental.map_and_batch(\n",
    "       map_func=parser1,\n",
    "       batch_size=batch_size))\n",
    "dataset1 = dataset1.shuffle(buffer_size=buffersize)\n",
    "\n",
    "subset = 'filenum'\n",
    "files2 = tf.matching_files(os.path.normpath(os.path.join(tfrecord_path, '%s*' % subset)))\n",
    "shards2 = tf.data.Dataset.from_tensor_slices(files2)\n",
    "# shards = shards.shuffle(tf.cast(tf.shape(files)[0], tf.int64))\n",
    "shards2 = shards2.repeat()\n",
    "# dataset = shards.interleave(tf.data.TFRecordDataset, cycle_length=20)\n",
    "dataset2 = tf.data.TFRecordDataset(shards2)\n",
    "#it reads from 6 files simoltaneously\n",
    "# dataset = dataset.shuffle(buffer_size=buffersize)\n",
    "parser2 = _parse_function2\n",
    "dataset2 = dataset2.apply(\n",
    "   tf.data.experimental.map_and_batch(\n",
    "       map_func=parser2,\n",
    "       batch_size=batch_size))\n",
    "dataset2 = dataset2.shuffle(buffer_size=buffersize)\n",
    "\n",
    "\n",
    "tfrecord_path_val = os.path.normpath(os.path.join(Data_path,tfrecord_folder_parent,tfrecord_val_folder))\n",
    "sorted_names_val = natsorted(os.listdir(tfrecord_path_val))\n",
    "trainfilenames_val = []\n",
    "for i in sorted_names_val:\n",
    "    trainfilenames_val.append(os.path.normpath(os.path.join(tfrecord_path_val,i)))\n",
    "\n",
    "dataset_val = tf.data.TFRecordDataset(trainfilenames_val)\n",
    "dataset_val = dataset_val.map(_parse_function)  # Parse the record into tensors.\n",
    "dataset_val = dataset_val.repeat()  # Repeat the input indefinitely.\n",
    "dataset_val = dataset_val.batch(batch_size)\n",
    "\n",
    "generator = TimeseriesGenerator(dataset1, dataset2, length=30, batch_size=batch_size)\n",
    "\n",
    "epochs_num = 50\n",
    "\n",
    "files = glob.glob(os.path.join(result_path,\"checkpoints\",ckpt_folder,'*'))\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "if not os.path.exists(os.path.join(result_path,\"checkpoints\",ckpt_folder)):\n",
    "    os.makedirs(os.path.join(result_path,\"checkpoints\",ckpt_folder))\n",
    "\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "print(\"initialized\")\n",
    "checkpoint_path = os.path.normpath(os.path.join(result_path,\"checkpoints\",ckpt_folder,\"weights.{epoch:02d}.hdf5\"))\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path, verbose=1, save_weights_only=True)\n",
    "\n",
    "opt = tf.keras.optimizers.SGD()\n",
    "\n",
    "\n",
    "\n",
    "# class MI\n",
    "# custom_loss = MI.compute_loss\n",
    "model.compile(loss='mse', optimizer=opt)\n",
    "\n",
    "history = model.fit( X,Y, steps_per_epoch=steps,epochs=epochs_num,callbacks = [cp_callback],\n",
    "                    shuffle=False,verbose=1,validation_data=dataset_val,validation_steps=val_steps)\n",
    "\n",
    "# model.fit_generator(generator, steps_per_epoch=steps, epochs=50, verbose=1)\n",
    "\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(os.path.normpath(os.path.join(result_path, 'models', \"model_\"+ckpt_folder+\".json\")), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(os.path.normpath(os.path.join(result_path, 'models', \"model_\"+ckpt_folder+\".h5\")))\n",
    "print(\"Saved model to disk\")\n",
    "# h5f.close()   \n",
    "print(datetime.datetime.now())\n",
    "%matplotlib inline\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.savefig(os.path.normpath(os.path.join(result_path,'images',ckpt_folder+'.png')))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "colab_type": "code",
    "id": "LlxVCJOL2lBP",
    "outputId": "2a7f3245-8cc9-4053-abac-c4ffed5d3ea0"
   },
   "outputs": [],
   "source": [
    "dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "coDH4cCQ8XDl"
   },
   "source": [
    "##train_crossval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 781
    },
    "colab_type": "code",
    "id": "emDox6U08VWu",
    "outputId": "2e5b31dc-a2a4-4907-8663-e22e4aa2eae5"
   },
   "outputs": [],
   "source": [
    "# import libraries.\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "tf.enable_eager_execution()\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import h5py\n",
    "import glob\n",
    "######################\n",
    "#import libraries.\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import librosa\n",
    "from librosa.core import stft, istft\n",
    "####import sounddevice as sd\n",
    "import time\n",
    "print('imported')\n",
    "\n",
    "result_path = '/content/drive/My Drive/thesis/results'\n",
    "# result_path = os.path.normpath(os.path.join(Data_path,'results'))\n",
    "\n",
    "seed = 7\n",
    "from tensorflow.keras.layers import Activation\n",
    "np.random.seed(seed)\n",
    "act1 = layers.LeakyReLU(alpha=0.1)\n",
    "model = Sequential()\n",
    "model.add(Dense(h[0], input_dim = w*len_data[1], kernel_initializer= 'lecun_uniform', bias_initializer = 'lecun_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('tanh'))\n",
    "act2=layers.LeakyReLU(alpha=0.1)\n",
    "model.add(Dense(h[1], kernel_initializer= 'lecun_uniform', bias_initializer = 'lecun_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(act2)\n",
    "act3=layers.LeakyReLU(alpha=0.1)\n",
    "model.add(Dense(h[2], kernel_initializer= 'lecun_uniform', bias_initializer = 'lecun_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(act3)\n",
    "model.add(Dense(len_data[1], kernel_initializer= 'lecun_uniform', bias_initializer = 'lecun_uniform'))\n",
    "#############################################\n",
    "import os\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "    features = {\"X\": tf.FixedLenFeature((w*257), tf.float32),\n",
    "              \"Y\": tf.FixedLenFeature((257), tf.float32)}\n",
    "    parsed_features = tf.parse_single_example(example_proto, features)\n",
    "    return parsed_features[\"X\"], parsed_features[\"Y\"]\n",
    "\n",
    "epochs_num = 1\n",
    "epochs_num_full = 1\n",
    "print(datetime.datetime.now())\n",
    "opt = tf.keras.optimizers.Adam()\n",
    "##############################################1st fold###############################################\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "tfrecord_path = os.path.normpath(os.path.join(Data_path,tfrecord_folder_parent,tfrecord_folder))\n",
    "subset = 'filenum'\n",
    "files = tf.matching_files(os.path.normpath(os.path.join(tfrecord_path, '%s*' % subset)))[0:64]\n",
    "shards = tf.data.Dataset.from_tensor_slices(files)\n",
    "shards = shards.shuffle(tf.cast(tf.shape(files)[0], tf.int64))\n",
    "shards = shards.repeat()\n",
    "dataset = shards.interleave(tf.data.TFRecordDataset, cycle_length=20)\n",
    "dataset = dataset.shuffle(buffer_size=buffersize)\n",
    "parser = _parse_function\n",
    "dataset = dataset.apply(\n",
    "   tf.data.experimental.map_and_batch(\n",
    "       map_func=parser,\n",
    "       batch_size=batch_size))\n",
    "dataset = dataset.prefetch(batch_size)\n",
    "\n",
    "tfrecord_path_val = os.path.normpath(os.path.join(Data_path,tfrecord_folder_parent,tfrecord_folder))\n",
    "subset = 'filenum'\n",
    "files_val = tf.matching_files(os.path.normpath(os.path.join(tfrecord_path_val, '%s*' % subset)))[64:79]\n",
    "shards_val = tf.data.Dataset.from_tensor_slices(files_val)\n",
    "shards_val = shards_val.repeat()\n",
    "dataset_val = shards_val.interleave(tf.data.TFRecordDataset, cycle_length=1)\n",
    "parser = _parse_function\n",
    "dataset_val = dataset_val.apply(\n",
    "   tf.data.experimental.map_and_batch(\n",
    "       map_func=parser,\n",
    "       batch_size=batch_size))\n",
    "dataset_val = dataset_val.prefetch(batch_size)\n",
    "\n",
    "model.compile(loss='mse', optimizer=opt)\n",
    "history = model.fit( dataset, steps_per_epoch=steps_kfold,epochs=epochs_num, verbose=1,validation_data=dataset_val,validation_steps=val_steps_kfold)\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(os.path.normpath(os.path.join(result_path, 'models', \"model_cross_1.json\")), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(os.path.normpath(os.path.join(result_path, 'models', \"model_cross_1.h5\")))\n",
    "print(\"Saved model to disk\") \n",
    "print(datetime.datetime.now())\n",
    "\n",
    "%matplotlib inline\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.savefig(os.path.normpath(os.path.join(result_path,'images','cross_1.png')))\n",
    "plt.show()\n",
    "##############################################2nd fold###############################################\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "weights = model.get_weights()\n",
    "\n",
    "tfrecord_path = os.path.normpath(os.path.join(Data_path,tfrecord_folder_parent,tfrecord_folder))\n",
    "subset = 'filenum'\n",
    "files = tf.matching_files(os.path.normpath(os.path.join(tfrecord_path, '%s*' % subset)))\n",
    "files = files[16:79]\n",
    "shards = tf.data.Dataset.from_tensor_slices(files)\n",
    "shards = shards.shuffle(tf.cast(tf.shape(files)[0], tf.int64))\n",
    "shards = shards.repeat()\n",
    "dataset = shards.interleave(tf.data.TFRecordDataset, cycle_length=20)\n",
    "dataset = dataset.shuffle(buffer_size=buffersize)\n",
    "parser = _parse_function\n",
    "dataset = dataset.apply(\n",
    "   tf.data.experimental.map_and_batch(\n",
    "       map_func=parser,\n",
    "       batch_size=batch_size))\n",
    "dataset = dataset.prefetch(batch_size)\n",
    "\n",
    "tfrecord_path_val = os.path.normpath(os.path.join(Data_path,tfrecord_folder_parent,tfrecord_folder))\n",
    "subset = 'filenum'\n",
    "files_val = tf.matching_files(os.path.normpath(os.path.join(tfrecord_path_val, '%s*' % subset)))[0:16]\n",
    "shards_val = tf.data.Dataset.from_tensor_slices(files_val)\n",
    "shards_val = shards_val.repeat()\n",
    "dataset_val = shards_val.interleave(tf.data.TFRecordDataset, cycle_length=1)\n",
    "parser = _parse_function\n",
    "dataset_val = dataset_val.apply(\n",
    "   tf.data.experimental.map_and_batch(\n",
    "       map_func=parser,\n",
    "       batch_size=batch_size))\n",
    "dataset_val = dataset_val.prefetch(batch_size)\n",
    "\n",
    "act1 = layers.LeakyReLU(alpha=0.1)\n",
    "model = Sequential()\n",
    "model.add(Dense(h[0], input_dim = w*len_data[1], kernel_initializer= tf.constant_initializer(weights[0]), bias_initializer = tf.constant_initializer(weights[1])))\n",
    "model.add(BatchNormalization(beta_initializer= tf.constant_initializer(weights[3]),gamma_initializer= tf.constant_initializer(weights[2]),moving_mean_initializer=tf.constant_initializer(weights[4]),moving_variance_initializer=tf.constant_initializer(weights[5])))\n",
    "model.add(Activation('tanh'))\n",
    "act2=layers.LeakyReLU(alpha=0.1)\n",
    "model.add(Dense(h[1], kernel_initializer= tf.constant_initializer(weights[6]), bias_initializer = tf.constant_initializer(weights[7])))\n",
    "model.add(BatchNormalization(beta_initializer= tf.constant_initializer(weights[9]),gamma_initializer= tf.constant_initializer(weights[8]),moving_mean_initializer=tf.constant_initializer(weights[10]),moving_variance_initializer=tf.constant_initializer(weights[11])))\n",
    "model.add(act2)\n",
    "act3=layers.LeakyReLU(alpha=0.1)\n",
    "model.add(Dense(h[2], kernel_initializer= tf.constant_initializer(weights[12]), bias_initializer = tf.constant_initializer(weights[13])))\n",
    "model.add(BatchNormalization(beta_initializer= tf.constant_initializer(weights[15]),gamma_initializer= tf.constant_initializer(weights[14]),moving_mean_initializer=tf.constant_initializer(weights[16]),moving_variance_initializer=tf.constant_initializer(weights[17])))\n",
    "model.add(act3)\n",
    "model.add(Dense(len_data[1], kernel_initializer= tf.constant_initializer(weights[18]), bias_initializer = tf.constant_initializer(weights[19])))\n",
    "\n",
    "model.compile(loss='mse', optimizer=opt)\n",
    "history = model.fit( dataset, steps_per_epoch=steps_kfold,epochs=epochs_num, verbose=1,validation_data=dataset_val,validation_steps=val_steps_kfold)\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(os.path.normpath(os.path.join(result_path, 'models', \"model_cross_2.json\")), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(os.path.normpath(os.path.join(result_path, 'models', \"model_cross_2.h5\")))\n",
    "print(\"Saved model to disk\") \n",
    "print(datetime.datetime.now())\n",
    "\n",
    "%matplotlib inline\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.savefig(os.path.normpath(os.path.join(result_path,'images','cross_2.png')))\n",
    "plt.show()\n",
    "##############################################3rd fold###############################################\n",
    "\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "tfrecord_path = os.path.normpath(os.path.join(Data_path,tfrecord_folder_parent,tfrecord_folder))\n",
    "subset = 'filenum'\n",
    "files = tf.matching_files(os.path.normpath(os.path.join(tfrecord_path, '%s*' % subset)))\n",
    "files = np.concatenate((files[0:16], files[32:79]))\n",
    "shards = tf.data.Dataset.from_tensor_slices(files)\n",
    "shards = shards.shuffle(tf.cast(tf.shape(files)[0], tf.int64))\n",
    "shards = shards.repeat()\n",
    "dataset = shards.interleave(tf.data.TFRecordDataset, cycle_length=20)\n",
    "dataset = dataset.shuffle(buffer_size=buffersize)\n",
    "parser = _parse_function\n",
    "dataset = dataset.apply(\n",
    "   tf.data.experimental.map_and_batch(\n",
    "       map_func=parser,\n",
    "       batch_size=batch_size))\n",
    "dataset = dataset.prefetch(batch_size)\n",
    "\n",
    "tfrecord_path_val = os.path.normpath(os.path.join(Data_path,tfrecord_folder_parent,tfrecord_folder))\n",
    "subset = 'filenum'\n",
    "files_val = tf.matching_files(os.path.normpath(os.path.join(tfrecord_path_val, '%s*' % subset)))[16:32]\n",
    "shards_val = tf.data.Dataset.from_tensor_slices(files_val)\n",
    "shards_val = shards_val.repeat()\n",
    "dataset_val = shards_val.interleave(tf.data.TFRecordDataset, cycle_length=1)\n",
    "parser = _parse_function\n",
    "dataset_val = dataset_val.apply(\n",
    "   tf.data.experimental.map_and_batch(\n",
    "       map_func=parser,\n",
    "       batch_size=batch_size))\n",
    "dataset_val = dataset_val.prefetch(batch_size)\n",
    "\n",
    "weights = model.get_weights()\n",
    "\n",
    "act1 = layers.LeakyReLU(alpha=0.1)\n",
    "model = Sequential()\n",
    "model.add(Dense(h[0], input_dim = w*len_data[1], kernel_initializer= tf.constant_initializer(weights[0]), bias_initializer = tf.constant_initializer(weights[1])))\n",
    "model.add(BatchNormalization(beta_initializer= tf.constant_initializer(weights[3]),gamma_initializer= tf.constant_initializer(weights[2]),moving_mean_initializer=tf.constant_initializer(weights[4]),moving_variance_initializer=tf.constant_initializer(weights[5])))\n",
    "model.add(Activation('tanh'))\n",
    "act2=layers.LeakyReLU(alpha=0.1)\n",
    "model.add(Dense(h[1], kernel_initializer= tf.constant_initializer(weights[6]), bias_initializer = tf.constant_initializer(weights[7])))\n",
    "model.add(BatchNormalization(beta_initializer= tf.constant_initializer(weights[9]),gamma_initializer= tf.constant_initializer(weights[8]),moving_mean_initializer=tf.constant_initializer(weights[10]),moving_variance_initializer=tf.constant_initializer(weights[11])))\n",
    "model.add(act2)\n",
    "act3=layers.LeakyReLU(alpha=0.1)\n",
    "model.add(Dense(h[2], kernel_initializer= tf.constant_initializer(weights[12]), bias_initializer = tf.constant_initializer(weights[13])))\n",
    "model.add(BatchNormalization(beta_initializer= tf.constant_initializer(weights[15]),gamma_initializer= tf.constant_initializer(weights[14]),moving_mean_initializer=tf.constant_initializer(weights[16]),moving_variance_initializer=tf.constant_initializer(weights[17])))\n",
    "model.add(act3)\n",
    "model.add(Dense(len_data[1], kernel_initializer= tf.constant_initializer(weights[18]), bias_initializer = tf.constant_initializer(weights[19])))\n",
    "\n",
    "model.compile(loss='mse', optimizer=opt)\n",
    "history = model.fit( dataset, steps_per_epoch=steps_kfold,epochs=epochs_num, verbose=1,validation_data=dataset_val,validation_steps=val_steps_kfold)\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(os.path.normpath(os.path.join(result_path, 'models', \"model_cross_3.json\")), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(os.path.normpath(os.path.join(result_path, 'models', \"model_cross_3.h5\")))\n",
    "print(\"Saved model to disk\") \n",
    "print(datetime.datetime.now())\n",
    "\n",
    "%matplotlib inline\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.savefig(os.path.normpath(os.path.join(result_path,'images','cross_3.png')))\n",
    "plt.show()\n",
    "##############################################4th fold###############################################\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "tfrecord_path = os.path.normpath(os.path.join(Data_path,tfrecord_folder_parent,tfrecord_folder))\n",
    "subset = 'filenum'\n",
    "files = tf.matching_files(os.path.normpath(os.path.join(tfrecord_path, '%s*' % subset)))\n",
    "files = np.concatenate((files[0:32], files[48:79]))\n",
    "shards = tf.data.Dataset.from_tensor_slices(files)\n",
    "shards = shards.shuffle(tf.cast(tf.shape(files)[0], tf.int64))\n",
    "shards = shards.repeat()\n",
    "dataset = shards.interleave(tf.data.TFRecordDataset, cycle_length=20)\n",
    "dataset = dataset.shuffle(buffer_size=buffersize)\n",
    "parser = _parse_function\n",
    "dataset = dataset.apply(\n",
    "   tf.data.experimental.map_and_batch(\n",
    "       map_func=parser,\n",
    "       batch_size=batch_size))\n",
    "dataset = dataset.prefetch(batch_size)\n",
    "\n",
    "tfrecord_path_val = os.path.normpath(os.path.join(Data_path,tfrecord_folder_parent,tfrecord_folder))\n",
    "subset = 'filenum'\n",
    "files_val = tf.matching_files(os.path.normpath(os.path.join(tfrecord_path_val, '%s*' % subset)))[32:48]\n",
    "shards_val = tf.data.Dataset.from_tensor_slices(files_val)\n",
    "shards_val = shards_val.repeat()\n",
    "dataset_val = shards_val.interleave(tf.data.TFRecordDataset, cycle_length=1)\n",
    "parser = _parse_function\n",
    "dataset_val = dataset_val.apply(\n",
    "   tf.data.experimental.map_and_batch(\n",
    "       map_func=parser,\n",
    "       batch_size=batch_size))\n",
    "dataset_val = dataset_val.prefetch(batch_size)\n",
    "\n",
    "weights = model.get_weights()\n",
    "\n",
    "act1 = layers.LeakyReLU(alpha=0.1)\n",
    "model = Sequential()\n",
    "model.add(Dense(h[0], input_dim = w*len_data[1], kernel_initializer= tf.constant_initializer(weights[0]), bias_initializer = tf.constant_initializer(weights[1])))\n",
    "model.add(BatchNormalization(beta_initializer= tf.constant_initializer(weights[3]),gamma_initializer= tf.constant_initializer(weights[2]),moving_mean_initializer=tf.constant_initializer(weights[4]),moving_variance_initializer=tf.constant_initializer(weights[5])))\n",
    "model.add(Activation('tanh'))\n",
    "act2=layers.LeakyReLU(alpha=0.1)\n",
    "model.add(Dense(h[1], kernel_initializer= tf.constant_initializer(weights[6]), bias_initializer = tf.constant_initializer(weights[7])))\n",
    "model.add(BatchNormalization(beta_initializer= tf.constant_initializer(weights[9]),gamma_initializer= tf.constant_initializer(weights[8]),moving_mean_initializer=tf.constant_initializer(weights[10]),moving_variance_initializer=tf.constant_initializer(weights[11])))\n",
    "model.add(act2)\n",
    "act3=layers.LeakyReLU(alpha=0.1)\n",
    "model.add(Dense(h[2], kernel_initializer= tf.constant_initializer(weights[12]), bias_initializer = tf.constant_initializer(weights[13])))\n",
    "model.add(BatchNormalization(beta_initializer= tf.constant_initializer(weights[15]),gamma_initializer= tf.constant_initializer(weights[14]),moving_mean_initializer=tf.constant_initializer(weights[16]),moving_variance_initializer=tf.constant_initializer(weights[17])))\n",
    "model.add(act3)\n",
    "model.add(Dense(len_data[1], kernel_initializer= tf.constant_initializer(weights[18]), bias_initializer = tf.constant_initializer(weights[19])))\n",
    "\n",
    "model.compile(loss='mse', optimizer=opt)\n",
    "history = model.fit( dataset, steps_per_epoch=steps_kfold,epochs=epochs_num, verbose=1,validation_data=dataset_val,validation_steps=val_steps_kfold)\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(os.path.normpath(os.path.join(result_path, 'models', \"model_cross_4.json\")), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(os.path.normpath(os.path.join(result_path, 'models', \"model_cross_4.h5\")))\n",
    "print(\"Saved model to disk\") \n",
    "print(datetime.datetime.now())\n",
    "\n",
    "%matplotlib inline\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.savefig(os.path.normpath(os.path.join(result_path,'images','cross_4.png')))\n",
    "plt.show()\n",
    "##############################################5th fold###############################################\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "tfrecord_path = os.path.normpath(os.path.join(Data_path,tfrecord_folder_parent,tfrecord_folder))\n",
    "subset = 'filenum'\n",
    "files = tf.matching_files(os.path.normpath(os.path.join(tfrecord_path, '%s*' % subset)))\n",
    "files = np.concatenate((files[0:48], files[64:79]))\n",
    "shards = tf.data.Dataset.from_tensor_slices(files)\n",
    "shards = shards.shuffle(tf.cast(tf.shape(files)[0], tf.int64))\n",
    "shards = shards.repeat()\n",
    "dataset = shards.interleave(tf.data.TFRecordDataset, cycle_length=20)\n",
    "dataset = dataset.shuffle(buffer_size=buffersize)\n",
    "parser = _parse_function\n",
    "dataset = dataset.apply(\n",
    "   tf.data.experimental.map_and_batch(\n",
    "       map_func=parser,\n",
    "       batch_size=batch_size))\n",
    "dataset = dataset.prefetch(batch_size)\n",
    "\n",
    "tfrecord_path_val = os.path.normpath(os.path.join(Data_path,tfrecord_folder_parent,tfrecord_folder))\n",
    "subset = 'filenum'\n",
    "files_val = tf.matching_files(os.path.normpath(os.path.join(tfrecord_path_val, '%s*' % subset)))[48:64]\n",
    "shards_val = tf.data.Dataset.from_tensor_slices(files_val)\n",
    "shards_val = shards_val.repeat()\n",
    "dataset_val = shards_val.interleave(tf.data.TFRecordDataset, cycle_length=1)\n",
    "parser = _parse_function\n",
    "dataset_val = dataset_val.apply(\n",
    "   tf.data.experimental.map_and_batch(\n",
    "       map_func=parser,\n",
    "       batch_size=batch_size))\n",
    "dataset_val = dataset_val.prefetch(batch_size)\n",
    "\n",
    "weights = model.get_weights()\n",
    "\n",
    "act1 = layers.LeakyReLU(alpha=0.1)\n",
    "model = Sequential()\n",
    "model.add(Dense(h[0], input_dim = w*len_data[1], kernel_initializer= tf.constant_initializer(weights[0]), bias_initializer = tf.constant_initializer(weights[1])))\n",
    "model.add(BatchNormalization(beta_initializer= tf.constant_initializer(weights[3]),gamma_initializer= tf.constant_initializer(weights[2]),moving_mean_initializer=tf.constant_initializer(weights[4]),moving_variance_initializer=tf.constant_initializer(weights[5])))\n",
    "model.add(Activation('tanh'))\n",
    "act2=layers.LeakyReLU(alpha=0.1)\n",
    "model.add(Dense(h[1], kernel_initializer= tf.constant_initializer(weights[6]), bias_initializer = tf.constant_initializer(weights[7])))\n",
    "model.add(BatchNormalization(beta_initializer= tf.constant_initializer(weights[9]),gamma_initializer= tf.constant_initializer(weights[8]),moving_mean_initializer=tf.constant_initializer(weights[10]),moving_variance_initializer=tf.constant_initializer(weights[11])))\n",
    "model.add(act2)\n",
    "act3=layers.LeakyReLU(alpha=0.1)\n",
    "model.add(Dense(h[2], kernel_initializer= tf.constant_initializer(weights[12]), bias_initializer = tf.constant_initializer(weights[13])))\n",
    "model.add(BatchNormalization(beta_initializer= tf.constant_initializer(weights[15]),gamma_initializer= tf.constant_initializer(weights[14]),moving_mean_initializer=tf.constant_initializer(weights[16]),moving_variance_initializer=tf.constant_initializer(weights[17])))\n",
    "model.add(act3)\n",
    "model.add(Dense(len_data[1], kernel_initializer= tf.constant_initializer(weights[18]), bias_initializer = tf.constant_initializer(weights[19])))\n",
    "\n",
    "model.compile(loss='mse', optimizer=opt)\n",
    "history = model.fit( dataset, steps_per_epoch=steps_kfold,epochs=epochs_num, verbose=1,validation_data=dataset_val,validation_steps=val_steps_kfold)\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(os.path.normpath(os.path.join(result_path, 'models', \"model_cross_5.json\")), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(os.path.normpath(os.path.join(result_path, 'models', \"model_cross_5.h5\")))\n",
    "print(\"Saved model to disk\") \n",
    "print(datetime.datetime.now())\n",
    "\n",
    "%matplotlib inline\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.savefig(os.path.normpath(os.path.join(result_path,'images','cross_5.png')))\n",
    "plt.show()\n",
    "############################################whole model#############################################\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "tfrecord_path = os.path.normpath(os.path.join(Data_path,tfrecord_folder_parent,tfrecord_folder))\n",
    "subset = 'filenum'\n",
    "files = tf.matching_files(os.path.normpath(os.path.join(tfrecord_path, '%s*' % subset)))\n",
    "shards = tf.data.Dataset.from_tensor_slices(files)\n",
    "shards = shards.shuffle(tf.cast(tf.shape(files)[0], tf.int64))\n",
    "shards = shards.repeat()\n",
    "dataset = shards.interleave(tf.data.TFRecordDataset, cycle_length=20)\n",
    "dataset = dataset.shuffle(buffer_size=buffersize)\n",
    "parser = _parse_function\n",
    "dataset = dataset.apply(\n",
    "   tf.data.experimental.map_and_batch(\n",
    "       map_func=parser,\n",
    "       batch_size=batch_size))\n",
    "dataset = dataset.prefetch(batch_size)\n",
    "\n",
    "tfrecord_path_val = os.path.normpath(os.path.join(Data_path,tfrecord_folder_parent,tfrecord_val_folder))\n",
    "subset = 'filenum'\n",
    "files_val = tf.matching_files(os.path.normpath(os.path.join(tfrecord_path_val, '%s*' % subset)))\n",
    "shards_val = tf.data.Dataset.from_tensor_slices(files_val)\n",
    "shards_val = shards_val.repeat()\n",
    "dataset_val = shards_val.interleave(tf.data.TFRecordDataset, cycle_length=1)\n",
    "parser = _parse_function\n",
    "dataset_val = dataset_val.apply(\n",
    "   tf.data.experimental.map_and_batch(\n",
    "       map_func=parser,\n",
    "       batch_size=batch_size))\n",
    "dataset_val = dataset_val.prefetch(batch_size)\n",
    "\n",
    "weights = model.get_weights()\n",
    "\n",
    "act1 = layers.LeakyReLU(alpha=0.1)\n",
    "model = Sequential()\n",
    "model.add(Dense(h[0], input_dim = w*len_data[1], kernel_initializer= tf.constant_initializer(weights[0]), bias_initializer = tf.constant_initializer(weights[1])))\n",
    "model.add(BatchNormalization(beta_initializer= tf.constant_initializer(weights[3]),gamma_initializer= tf.constant_initializer(weights[2]),moving_mean_initializer=tf.constant_initializer(weights[4]),moving_variance_initializer=tf.constant_initializer(weights[5])))\n",
    "model.add(Activation('tanh'))\n",
    "act2=layers.LeakyReLU(alpha=0.1)\n",
    "model.add(Dense(h[1], kernel_initializer= tf.constant_initializer(weights[6]), bias_initializer = tf.constant_initializer(weights[7])))\n",
    "model.add(BatchNormalization(beta_initializer= tf.constant_initializer(weights[9]),gamma_initializer= tf.constant_initializer(weights[8]),moving_mean_initializer=tf.constant_initializer(weights[10]),moving_variance_initializer=tf.constant_initializer(weights[11])))\n",
    "model.add(act2)\n",
    "act3=layers.LeakyReLU(alpha=0.1)\n",
    "model.add(Dense(h[2], kernel_initializer= tf.constant_initializer(weights[12]), bias_initializer = tf.constant_initializer(weights[13])))\n",
    "model.add(BatchNormalization(beta_initializer= tf.constant_initializer(weights[15]),gamma_initializer= tf.constant_initializer(weights[14]),moving_mean_initializer=tf.constant_initializer(weights[16]),moving_variance_initializer=tf.constant_initializer(weights[17])))\n",
    "model.add(act3)\n",
    "model.add(Dense(len_data[1], kernel_initializer= tf.constant_initializer(weights[18]), bias_initializer = tf.constant_initializer(weights[19])))\n",
    "\n",
    "files = glob.glob(os.path.join(result_path,\"checkpoints\",ckpt_folder,'*'))\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "if not os.path.exists(os.path.join(result_path,\"checkpoints\",ckpt_folder)):\n",
    "    os.makedirs(os.path.join(result_path,\"checkpoints\",ckpt_folder))\n",
    "print(datetime.datetime.now())\n",
    "print(\"initialized\")\n",
    "checkpoint_path = os.path.normpath(os.path.join(result_path,\"checkpoints\",ckpt_folder,\"weights.{epoch:02d}.hdf5\"))\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path, verbose=1, save_weights_only=True)\n",
    "\n",
    "model.compile(loss='mse', optimizer=opt)\n",
    "history = model.fit( dataset, steps_per_epoch=steps,epochs=epochs_num_full,callbacks = [cp_callback], verbose=1,validation_data=dataset_val,validation_steps=val_steps)\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(os.path.normpath(os.path.join(result_path, 'models', \"model_cross_whole.json\")), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(os.path.normpath(os.path.join(result_path, 'models', \"model_cross_whole.h5\")))\n",
    "print(\"Saved model to disk\") \n",
    "print(datetime.datetime.now())\n",
    "\n",
    "%matplotlib inline\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.savefig(os.path.normpath(os.path.join(result_path,'images','cross_whole.png')))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Me3TYC-qLma"
   },
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(os.path.normpath(os.path.join(result_path, 'models', \"model_\"+ckpt_folder+\".json\")), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(os.path.normpath(os.path.join(result_path, 'models', \"model_\"+ckpt_folder+\".h5\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h4dPZXjUqLmd"
   },
   "source": [
    "### archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "62VUJx_rqLmd",
    "outputId": "76a8d083-03c6-4eef-c42f-720222a40859"
   },
   "outputs": [],
   "source": [
    "# model.add(Dense(h[0], input_dim = w*len_data[1], kernel_initializer= tf.constant_initializer(weights[0]), bias_initializer = tf.constant_initializer(weights[1])))\n",
    "# , kernel_initializer= tf.constant_initializer(layer1[0]), bias_initializer = tf.constant_initializer(layer1[2])\n",
    "# tf.constant_initializer(layer1[0])\n",
    "# tf.constant_initializer(layer1[2])\n",
    "# model.add(BatchNormalization(beta_initializer= tf.constant_initializer(weights[3]),gamma_initializer= tf.constant_initializer(weights[2]),moving_mean_initializer=tf.constant_initializer(weights[4]),moving_variance_initializer=tf.constant_initializer(weights[5])))\n",
    "# model.add(Activation('sigmoid'))\n",
    "# model.add(Dense(h[1], kernel_initializer= tf.constant_initializer(weights[2]), bias_initializer = tf.constant_initializer(weights[3])))\n",
    "\n",
    "# model.add(BatchNormalization(beta_initializer= tf.constant_initializer(weights[9]),gamma_initializer= tf.constant_initializer(weights[8]),moving_mean_initializer=tf.constant_initializer(weights[10]),moving_variance_initializer=tf.constant_initializer(weights[11])))\n",
    "\n",
    "# model.add(Activation('tanh'))\n",
    "\n",
    "# model.add(Activation('sigmoid'))\n",
    "\n",
    "# model.add(Dense(h[2]))\n",
    "# model.add(act3)\n",
    "# act=layers.LeakyReLU(alpha=0.01)\n",
    "# model.add(Dense(h[2], kernel_initializer= tf.constant_initializer(weights[4]), bias_initializer = tf.constant_initializer(weights[5])))\n",
    "\n",
    "# model.add(Activation('tanh'))\n",
    "# model.add(BatchNormalization(beta_initializer= tf.constant_initializer(weights[15]),gamma_initializer= tf.constant_initializer(weights[14]),moving_mean_initializer=tf.constant_initializer(weights[16]),moving_variance_initializer=tf.constant_initializer(weights[17])))\n",
    "\n",
    "# model.add(Activation('sigmoid'))\n",
    "# act4=layers.LeakyReLU(alpha=0.1)\n",
    "# model.add(Activation('sigmoid'))\n",
    "# model.add(layers.Dropout(rate, noise_shape=None, seed=None))\n",
    "# model.add(Dense(h[2]))\n",
    "# model.add(act3)\n",
    "# act=layers.LeakyReLU(alpha=0.01)\n",
    "# model.add(Dense(h[3], kernel_initializer= tf.constant_initializer(weights[18]), bias_initializer = tf.constant_initializer(weights[19])))\n",
    "# model.add(Dense(h[3], kernel_initializer= 'lecun_uniform', bias_initializer = 'lecun_uniform'))\n",
    "# model.add(Activation('tanh'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(BatchNormalization(beta_initializer= tf.constant_initializer(weights[21]),gamma_initializer= tf.constant_initializer(weights[20]),moving_mean_initializer=tf.constant_initializer(weights[22]),moving_variance_initializer=tf.constant_initializer(weights[23])))\n",
    "# model.add(act4)\n",
    "# model.add(Dense(len_data[1], kernel_initializer= tf.constant_initializer(weights[6]), bias_initializer = tf.constant_initializer(weights[7])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2TcCyc4u0tII"
   },
   "outputs": [],
   "source": [
    "def mel_filter(dft_signal):\n",
    "    sample_rate = tf.constant(16000, dtype =tf.float32)\n",
    "    nfilt = tf.constant(40, dtype =tf.int32)\n",
    "    NFFT = tf.constant(512, dtype =tf.float32)\n",
    "    low_freq_mel = tf.constant(0, dtype =tf.float32)\n",
    "    log_10 = tf.log(1 + (sample_rate / 2) / 700)/tf.log(tf.constant(10, dtype =tf.float32))\n",
    "    high_freq_mel = 2595 * log_10  # Convert Hz to Mel\n",
    "    print(\"here2\")\n",
    "    mel_points = tf.linspace(low_freq_mel, high_freq_mel, nfilt+2)  # Equally spaced in Mel scale\n",
    "    print(\"here3\")\n",
    "    hz_points = 700 * (10**(mel_points / 2595) - 1)  # Convert Mel to Hz\n",
    "    print(\"here4\")\n",
    "    binf = tf.floor((NFFT + 1) * hz_points / sample_rate)\n",
    "    print(\"here4\")\n",
    "    def trans_1(k):\n",
    "        global fbank\n",
    "        fbank[m-1,k] = (k - bin[m - 1]) / (bin[m] - bin[m - 1])\n",
    "        return fbank\n",
    "\n",
    "    def trans_2(k):\n",
    "        global fbank\n",
    "        fbank[m-1,k] = (bin[m + 1] - k) / (bin[m + 1] - bin[m])\n",
    "        return fbank\n",
    "\n",
    "    def wrapper_1(x):\n",
    "        return tf.py_function(trans_1, [x], tf.float32)\n",
    "\n",
    "    def wrapper_2(x):\n",
    "        return tf.py_function(trans_2, [x], tf.float32)\n",
    "    print(\"here5\")\n",
    "    fbank = tf.zeros((nfilt, tf.constant(257,dtype=tf.int32)))\n",
    "    print(\"here6\")\n",
    "    for m in range(1, 41):\n",
    "        print(\"here7\")\n",
    "        f_m_minus = binf[m - 1]  # left\n",
    "        f_m = binf[m]           # center\n",
    "        f_m_plus = binf[m + 1]    # right\n",
    "        print(\"here7\")\n",
    "        tf.map_fn(wrapper_1, tf.range(f_m_minus, f_m))\n",
    "        print(\"here7\")\n",
    "        tf.map_fn(wrapper_2, tf.range(f_m, f_m_plus))\n",
    "    filter_banks = tf.matmul(dft_signal, tf.transpose(fbank))\n",
    "    print(\"1\")\n",
    "    mask = tf.cast(tf.equal(filter_banks,0),dtype=tf.float32)\n",
    "    filter_banks = filter_banks + (mask*1e-7)\n",
    "#     filter_banks = tf.where(tf.equal(filter_banks,0), 1e-7, filter_banks)\n",
    "    print(\"2\")\n",
    "    return filter_banks\n",
    "\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    corr = tf.zeros((40,))\n",
    "#     def compute_all(x,k1,k2):\n",
    "#         c[x] = tf.contrib.metrics.streaming_pearson_correlation(k1[x],k2[x])[0]\n",
    "#         print(\"33\")\n",
    "# #         true_m = K.mean(y_true)\n",
    "# #         pred_m = K.mean(y_pred)\n",
    "# #         cov_coef = K.mean(tf.multiply(y_true-true_m,y_pred-pred_m))\n",
    "#         corr[x] = -0.5*(tf.log(1-c[x]**2))\n",
    "#         return corr\n",
    "\n",
    "    def corr1(x,k1,k2):\n",
    "        print(\"blah\")\n",
    "        c[x] = tf.contrib.metrics.streaming_pearson_correlation(k1[x],k2[x])[0]\n",
    "#         return tf.py_function(compute_all, [x,k1,k2], tf.float32)\n",
    "        corr[x] = -0.5*(tf.log(1-c[x]**2))\n",
    "        return corr\n",
    "    true = K.pow(tf.constant(10, dtype =tf.float32),y_true)\n",
    "    pred = K.pow(tf.constant(10, dtype =tf.float32),y_pred)\n",
    "    print(\"here\")\n",
    "    filtered_true = mel_filter(true)\n",
    "    filtered_pred = mel_filter(pred)\n",
    "    print(\"3\")\n",
    "    correl = tf.map_fn(corr1, [tf.range(128),filtered_true,filtered_pred])\n",
    "    print(\"4\")\n",
    "    mi = (1/40)*tf.sum(correl)\n",
    "    return mi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0zj-IzjaqLmf",
    "outputId": "49a0d7b9-aa9b-461e-a97a-85a7fb8f341b"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.savefig(os.path.normpath(os.path.join(result_path,'images',ckpt_folder+'.png')))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8pOlpP8BcTG2"
   },
   "source": [
    "# display how much ram is being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "colab_type": "code",
    "id": "997-79iZlRLW",
    "outputId": "2319aada-e2d5-4959-a94c-0bdfcdf5f4ff"
   },
   "outputs": [],
   "source": [
    "# memory footprint support libraries/code\n",
    "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
    "!pip install gputil\n",
    "!pip install psutil\n",
    "!pip install humanize\n",
    "import psutil\n",
    "import humanize\n",
    "import os\n",
    "import GPUtil as GPU\n",
    "GPUs = GPU.getGPUs()\n",
    "# XXX: only one GPU on Colab and isn’t guaranteed\n",
    "gpu = GPUs[0]\n",
    "def printm():\n",
    " process = psutil.Process(os.getpid())\n",
    " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
    " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
    "printm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oPTdFLrUchYm"
   },
   "source": [
    "# reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jyNWdUpRlRLb"
   },
   "outputs": [],
   "source": [
    "write_path = 'D:/studies/university/thesis/speech_separation_codes/du16/donesomestuff/10hdata'\n",
    "mixed_folder = os.path.normpath(os.path.join(write_path,'mixed_log_10h_norm'))\n",
    "h5f = h5py.File(mixed_folder+'.hdf5','r')\n",
    "ftr = h5f['mixed_log_10h_norm'][0:126]\n",
    "h5f.close()\n",
    "mixed_folder = os.path.normpath(os.path.join(write_path,'mixed_phase_10h_nozeroinsert'))\n",
    "h5f = h5py.File(mixed_folder+'.hdf5','r')\n",
    "ftr_phase = h5f['mixed_phase_10h_nozeroinsert'][0:126]\n",
    "h5f.close()\n",
    "mixed_folder = os.path.normpath(os.path.join(write_path,'mixed_log_10h_nozeroinsert'))\n",
    "h5f = h5py.File(mixed_folder+'.hdf5','r')\n",
    "ftr_raw = h5f['mixed_log_10h_nozeroinsert'][0:126]\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YEee-6XTlRLe"
   },
   "outputs": [],
   "source": [
    "oneframe_path = os.path.normpath(os.path.join(write_path,'oneframe_norm'))\n",
    "f = h5py.File(oneframe_path+'.hdf5', 'w')\n",
    "dd = f.create_dataset('oneframe_norm', data=ftr)\n",
    "f.close()\n",
    "nozeroinsert_path = os.path.normpath(os.path.join(write_path,'nozeroinsert'))\n",
    "f = h5py.File(nozeroinsert_path+'.hdf5', 'w')\n",
    "dd = f.create_dataset('nozeroinsert', data=ftr_phase)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ltO6lKclRLh",
    "outputId": "03676a3e-5806-4838-a1e8-0029bd7a0b4d"
   },
   "outputs": [],
   "source": [
    "pred = pretrained_model.predict(ftr)\n",
    "mean = np.loadtxt(os.path.normpath(os.path.join(write_path,'mean_mixed_log.txt')))\n",
    "std = np.loadtxt(os.path.normpath(os.path.join(write_path,'std_mixed_log.txt')))\n",
    "pred_unnorm = (pred*std)+mean\n",
    "recon = reconstruct(pred_unnorm, ftr_phase)\n",
    "import sounddevice as sd\n",
    "sd.play(recon,16000)\n",
    "\n",
    "Pxx, freqs, bins, im = plt.specgram(recon, NFFT=512, Fs=16000, noverlap=256)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yoeCe_X0lRLk"
   },
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "# sf.write('D:\\\\studies\\\\university\\\\thesis\\\\speech_separation_codes\\\\du16\\\\donesomestuff\\\\results\\\\rbm\\\\1\\\\3layer_tanh_vis_nobias.wav',recon,16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x0-eH8rBlRLm",
    "outputId": "c4e91b19-1315-4937-8d14-c603aa171cf8"
   },
   "outputs": [],
   "source": [
    "recon = reconstruct(hidden1_b, ftr_phase)\n",
    "import sounddevice as sd\n",
    "sd.play(recon,16000)\n",
    "Pxx, freqs, bins, im = plt.specgram(recon, NFFT=512, Fs=16000, noverlap=256)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BpFdb-SSlRLp",
    "outputId": "73dc1c7d-dec7-4fb1-8739-ba40549323d8"
   },
   "outputs": [],
   "source": [
    "recon = reconstruct(ftr_raw, ftr_phase)\n",
    "import sounddevice as sd\n",
    "sd.play(recon,16000)\n",
    "from scipy import signal\n",
    "# frequencies, times, spectrogram = signal.spectrogram(recon, 16000)\n",
    "# plt.pcolormesh(times, frequencies, spectrogram)\n",
    "# plt.imshow(spectrogram)\n",
    "# plt.ylabel('Frequency [Hz]')\n",
    "# plt.xlabel('Time [sec]')\n",
    "# plt.pcolormesh\n",
    "# plt.show()\n",
    "Pxx, freqs, bins, im = plt.specgram(recon, NFFT=512, Fs=16000, noverlap=256)\n",
    "# The `specgram` method returns 4 objects. They are:\n",
    "# - Pxx: the periodogram\n",
    "# - freqs: the frequency vector\n",
    "# - bins: the centers of the time bins\n",
    "# - im: the matplotlib.image.AxesImage instance representing the data in the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CnsK7Qj7lRLs",
    "outputId": "7f0911ef-57a9-4f4b-ae0a-ec6f169f4011"
   },
   "outputs": [],
   "source": [
    "import h5py \n",
    "import tensorflow as tf\n",
    "hh = h5py.File('ftr_refrmd_10h.hdf5', 'r')\n",
    "d=hh['ftr_refrmd_10h'][0]\n",
    "len_data=d.shape\n",
    "hh.close()\n",
    "len_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uNqqgL4ElRLv"
   },
   "outputs": [],
   "source": [
    "def reconstruct(wave,angle):\n",
    "    recon = np.sqrt(np.power(10, wave))\n",
    "    recon1 = recon*np.cos(angle)+recon*np.sin(angle)*1j\n",
    "    recon = librosa.core.istft((recon1.T), hop_length=256, win_length=512, window='hann')\n",
    "    return recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "5vzrh5-vlRLx",
    "outputId": "2081f2b0-1e57-4a5f-94e1-2ed5464c59d0"
   },
   "outputs": [],
   "source": [
    "val_len[0] // (batch_size*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mbX4c3JjlRL0"
   },
   "outputs": [],
   "source": [
    "# def custom_loss(y_true, y_pred):\n",
    "# #     ftr = h5f['mixed_log_30h_norm'][0:128]\n",
    "# #     ftr_tensor = tf.convert_to_tensor(ftr, dtype=tf.float32)\n",
    "#     coef = np.ones(257,)*0.1\n",
    "#     coef[12:24] = np.ones(12,)*0.3\n",
    "#     coef[24:48] = np.ones(24,)*0.5\n",
    "#     coef[48:96] = np.ones(48,)*1\n",
    "#     coef[96:160] = np.ones(64,)*0.8\n",
    "#     return K.mean(K.square((y_pred - y_true)*coef), axis=-1)\n",
    "################################################################################\n",
    "# def mel_filter(dft_signal):\n",
    "#     sample_rate = tf.constant(16000, dtype =tf.float32)\n",
    "#     nfilt = tf.constant(40, dtype =tf.int32)\n",
    "#     NFFT = tf.constant(512, dtype =tf.float32)\n",
    "#     low_freq_mel = tf.constant(0, dtype =tf.float32)\n",
    "#     log_10 = tf.log(1 + (sample_rate / 2) / 700)/tf.log(tf.constant(10, dtype =tf.float32))\n",
    "#     high_freq_mel = 2595 * log_10  # Convert Hz to Mel\n",
    "#     mel_points = tf.linspace(low_freq_mel, high_freq_mel, nfilt+2)  # Equally spaced in Mel scale\n",
    "#     hz_points = 700 * (10**(mel_points / 2595) - 1)  # Convert Mel to Hz\n",
    "#     binf = tf.floor((NFFT + 1) * hz_points / sample_rate)\n",
    "#     def wrapper_3(xm):\n",
    "#         def wrapper_1(x):\n",
    "# #             fbank[xm-1,x] = (x - bin[xm - 1]) / (bin[xm] - bin[xm - 1])\n",
    "# #             fbank[xm-1,x].assign(x - bin[xm - 1]) / (bin[xm] - bin[xm - 1])\n",
    "#             print(\"kkk\")\n",
    "\n",
    "#         def wrapper_2(x):\n",
    "#             return tf.assign(fbank[xm-1,x],(bin[xm + 1] - x) / (bin[xm + 1] - bin[xm]))\n",
    "        \n",
    "#         fbank = tf.zeros((nfilt, tf.constant(257,dtype=tf.int32)))\n",
    "#         f_m_minus = K.cast(binf[xm - 1],dtype=tf.int32)  # left\n",
    "#         f_m = K.cast(binf[xm],dtype=tf.int32)       # center\n",
    "#         f_m_plus = K.cast(binf[xm + 1],dtype=tf.int32)    # right\n",
    "#         print(tf.range(f_m_minus, f_m))\n",
    "#         print('dddd')\n",
    "#         fbank[xm-1,x] = tf.map_fn(wrapper_1, tf.range(f_m_minus, f_m))\n",
    "#         print('jjj')\n",
    "#         tf.map_fn(wrapper_2, tf.range(f_m, f_m_plus))\n",
    "#         return fbank\n",
    "#     start = tf.constant(1, dtype =tf.int32)\n",
    "#     stop = tf.constant(41, dtype =tf.int32)\n",
    "#     tf.map_fn(wrapper_3, tf.range(start,stop))\n",
    "#     filter_banks = tf.matmul(dft_signal, tf.transpose(fbank))\n",
    "#     mask = tf.cast(tf.equal(filter_banks,0),dtype=tf.float32)\n",
    "#     filter_banks = filter_banks + (mask*1e-7)\n",
    "# #     filter_banks = tf.where(tf.equal(filter_banks,0), 1e-7, filter_banks)\n",
    "#     return filter_banks\n",
    "\n",
    "\n",
    "# def custom_loss(y_true, y_pred):\n",
    "#     corr = tf.zeros((40,))\n",
    "# #     def compute_all(x,k1,k2):\n",
    "# #         c[x] = tf.contrib.metrics.streaming_pearson_correlation(k1[x],k2[x])[0]\n",
    "# #         print(\"33\")\n",
    "# # #         true_m = K.mean(y_true)\n",
    "# # #         pred_m = K.mean(y_pred)\n",
    "# # #         cov_coef = K.mean(tf.multiply(y_true-true_m,y_pred-pred_m))\n",
    "# #         corr[x] = -0.5*(tf.log(1-c[x]**2))\n",
    "# #         return corr\n",
    "#     def trans_3(k):\n",
    "#         global c\n",
    "#         print(\"i\")\n",
    "#         c[k] = -0.5*(tf.log(1-(tf.contrib.metrics.streaming_pearson_correlation(filtered_true[k],filtered_pred[k])[0])**2))\n",
    "# #         corr[x] = -0.5*(tf.log(1-c[x]**2))\n",
    "#         return c\n",
    "#     def corr1(x):\n",
    "#         print(\"dont\")\n",
    "#         return tf.py_function(trans_3,[x], tf.float32)\n",
    "#     true = K.pow(tf.constant(10, dtype =tf.float32),y_true)\n",
    "#     pred = K.pow(tf.constant(10, dtype =tf.float32),y_pred)\n",
    "#     filtered_true = mel_filter(true)\n",
    "#     print(\"i tots\")\n",
    "#     filtered_pred = mel_filter(pred)\n",
    "#     c = tf.zeros((tf.constant(1,dtype=tf.int32), tf.constant(128,dtype=tf.int32)))\n",
    "#     correl = tf.map_fn(corr1, [tf.range(128)])\n",
    "# #     correl = tf.map_fn(corr1, [tf.range(128),filtered_true,filtered_pred])\n",
    "#     mi = (1/40)*tf.sum(correl)\n",
    "#     return y_true-y_pred\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wIQNynUjkNOU"
   },
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    def mel_filter(dft_signal):\n",
    "        sample_rate = tf.constant(16000, dtype =tf.float32)\n",
    "        nfilt = tf.constant(40, dtype =tf.int32)\n",
    "        NFFT = tf.constant(512, dtype =tf.float32)\n",
    "        low_freq_mel = tf.constant(0, dtype =tf.float32)\n",
    "        log_10 = tf.log(1 + (sample_rate / 2) / 700)/tf.log(tf.constant(10, dtype =tf.float32))\n",
    "        high_freq_mel = 2595 * log_10  # Convert Hz to Mel\n",
    "        mel_points = tf.linspace(low_freq_mel, high_freq_mel, nfilt+2)  # Equally spaced in Mel scale\n",
    "        hz_points = 700 * (10**(mel_points / 2595) - 1)  # Convert Mel to Hz\n",
    "        binf = tf.floor((NFFT + 1) * hz_points / sample_rate)\n",
    "    #     def trans_1(k):\n",
    "    #         global fbank\n",
    "    #         fbank[m-1,k] = (k - bin[m - 1]) / (bin[m] - bin[m - 1])\n",
    "    #         return fbank\n",
    "\n",
    "    #     def trans_2(k):\n",
    "    #         global fbank\n",
    "    #         fbank[m-1,k] = (bin[m + 1] - k) / (bin[m + 1] - bin[m])\n",
    "    #         return fbank\n",
    "\n",
    "        def trans_1(kk,mm,bank,binf):\n",
    "#             global fbank\n",
    "            bank[mm-1,kk] = (kk - binf[mm - 1]) / (binf[mm] - binf[mm - 1])\n",
    "            return bank\n",
    "\n",
    "        def trans_2(kk,mm,bank,binf):\n",
    "#             global fbank\n",
    "            bank[mm-1,kk] = (binf[mm + 1] - kk) / (binf[mm + 1] - binf[mm])\n",
    "            return bank\n",
    "\n",
    "    #     def wrapper_1(x):\n",
    "    #         return tf.py_function(trans_1, [x], tf.float32)\n",
    "\n",
    "    #     def wrapper_2(x):\n",
    "    #         return tf.py_function(trans_2, [x], tf.float32)\n",
    "        fbank = tf.zeros((nfilt, tf.constant(257,dtype=tf.int32)))\n",
    "        for m in range(1, 41):\n",
    "            f_m_minus = binf[m - 1]  # left\n",
    "            f_m = binf[m]           # center\n",
    "            f_m_plus = binf[m + 1]    # right\n",
    "    #         tf.map_fn(wrapper_1, tf.range(f_m_minus, f_m))\n",
    "    #         tf.map_fn(wrapper_2, tf.range(f_m, f_m_plus))\n",
    "#             for k in tf.range(f_m_minus, f_m):\n",
    "#                 fbank[m - 1, k] = (k - binf[m - 1]) / (binf[m] - binf[m - 1])\n",
    "#             for k in tf.range(f_m, f_m_plus):\n",
    "#                 fbank[m - 1, k] = (binf[m + 1] - k) / (binf[m + 1] - binf[m])\n",
    "            fbank=tf.py_function(trans_1, [tf.range(f_m_minus, f_m),m,fbank,binf], tf.float32)\n",
    "            fbank=tf.py_function(trans_2, [tf.range(f_m, f_m_plus),m,fbank,binf], tf.float32)\n",
    "        \n",
    "        filter_banks = K.dot(dft_signal, tf.transpose(fbank))\n",
    "        print(fbank)\n",
    "        mask = K.cast(tf.equal(filter_banks,0),tf.float32)\n",
    "        filter_banks = filter_banks + (mask*1e-7)\n",
    "        return filter_banks\n",
    "\n",
    "    def trans_corr(k):\n",
    "        global c\n",
    "        c[k] = -0.5*(tf.log(1-tf.contrib.metrics.streaming_pearson_correlation(filtered_true[k],filtered_pred[k])[0]**2))\n",
    "#         c[k] = K.sum(y_pred)\n",
    "        return c\n",
    "\n",
    "    def wrapper_corr(x):\n",
    "        return tf.py_function(trans_corr, [x], tf.float32)\n",
    "    \n",
    "    true = K.pow(tf.constant(10, dtype =tf.float32),y_true)\n",
    "    print(true)\n",
    "    pred = K.pow(tf.constant(10, dtype =tf.float32),y_pred)\n",
    "    filtered_true = mel_filter(true)\n",
    "    print(filtered_true)\n",
    "    filtered_pred = mel_filter(pred)\n",
    "#     c = tf.zeros((tf.constant(40,dtype=tf.float32),1))\n",
    "#     tf.map_fn(wrapper_corr, tf.range(tf.constant(40, dtype =tf.float32)))\n",
    "#     mi = (1/40)*K.sum(c)\n",
    "    mi = K.mean(K.square(filtered_pred - filtered_true), axis=-1)\n",
    "    return mi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qe1huJMDokXW"
   },
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    def mel_filter(dft_signal):\n",
    "        sample_rate = tf.constant(16000, dtype =tf.float32)\n",
    "        nfilt = tf.constant(40, dtype =tf.int32)\n",
    "        NFFT = tf.constant(512, dtype =tf.float32)\n",
    "        low_freq_mel = tf.constant(0, dtype =tf.float32)\n",
    "        log_10 = tf.log(1 + (sample_rate / 2) / 700)/tf.log(tf.constant(10, dtype =tf.float32))\n",
    "        high_freq_mel = 2595 * log_10  # Convert Hz to Mel\n",
    "        mel_points = tf.linspace(low_freq_mel, high_freq_mel, nfilt+2)  # Equally spaced in Mel scale\n",
    "        hz_points = 700 * (10**(mel_points / 2595) - 1)  # Convert Mel to Hz\n",
    "        binf = tf.floor((NFFT + 1) * hz_points / sample_rate)\n",
    "\n",
    "        def trans_1(kk,mm,bank,binf):\n",
    "#             global fbank\n",
    "            bank[mm-1,kk] = (kk - binf[mm - 1]) / (binf[mm] - binf[mm - 1])\n",
    "            return bank\n",
    "\n",
    "        def trans_2(kk,mm,bank,binf):\n",
    "#             global fbank\n",
    "            bank[mm-1,kk] = (binf[mm + 1] - kk) / (binf[mm + 1] - binf[mm])\n",
    "            return bank\n",
    "\n",
    "        fbank = tf.zeros((nfilt, tf.constant(257,dtype=tf.int32)))\n",
    "        for m in range(1, 41):\n",
    "            f_m_minus = binf[m - 1]  # left\n",
    "            f_m = binf[m]           # center\n",
    "            f_m_plus = binf[m + 1]    # right\n",
    "\n",
    "            fbank=tf.py_function(trans_1, [tf.range(f_m_minus, f_m),m,fbank,binf], tf.float32)\n",
    "            fbank=tf.py_function(trans_2, [tf.range(f_m, f_m_plus),m,fbank,binf], tf.float32)\n",
    "        filter_banks = K.dot(dft_signal, K.transpose(fbank))\n",
    "        print(fbank)\n",
    "        mask = K.cast(tf.equal(filter_banks,0),tf.float32)\n",
    "        filter_banks = filter_banks + (mask*1e-7)\n",
    "        return filter_banks\n",
    "\n",
    "    def trans_corr(k):\n",
    "        global c\n",
    "        c[k] = -0.5*(tf.log(1-tf.contrib.metrics.streaming_pearson_correlation(filtered_true[k],filtered_pred[k])[0]**2))\n",
    "#         c[k] = K.sum(y_pred)\n",
    "        return c\n",
    "\n",
    "    def wrapper_corr(x):\n",
    "        return tf.py_function(trans_corr, [x], tf.float32)\n",
    "    \n",
    "    true = K.pow(tf.constant(10, dtype=tf.float32),y_true)\n",
    "    print(true)\n",
    "    pred = K.pow(tf.constant(10, dtype=tf.float32),y_pred)\n",
    "    filtered_true = mel_filter(true)\n",
    "    print(filtered_true)\n",
    "    filtered_pred = mel_filter(pred)\n",
    "#     c = tf.zeros((tf.constant(40,dtype=tf.float32),1))\n",
    "#     tf.map_fn(wrapper_corr, tf.range(tf.constant(40, dtype =tf.float32)))\n",
    "#     mi = (1/40)*K.sum(c)\n",
    "    mi = K.mean(K.square(filtered_pred - filtered_true), axis=-1)\n",
    "    return mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RFseFyi_U9g4"
   },
   "outputs": [],
   "source": [
    "#             while_condition = lambda f_m_minus: tf.less(f_m_minus, f_m) and tf.greater(f_m_minus)\n",
    "#             i = tf.constant(f_m_minus, tf.float32)\n",
    "#             j = tf.constant(f_m, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FOrIcBoXdoWm"
   },
   "outputs": [],
   "source": [
    "# import libraries.\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "# tf.enable_eager_execution()\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import h5py\n",
    "import glob\n",
    "######################\n",
    "#import libraries.\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import librosa\n",
    "from librosa.core import stft, istft\n",
    "####import sounddevice as sd\n",
    "import time\n",
    "print('imported')\n",
    "\n",
    "result_path = '/content/drive/My Drive/thesis/results'\n",
    "# result_path = os.path.normpath(os.path.join(Data_path,'results'))\n",
    "\n",
    "seed = 7\n",
    "from tensorflow.keras.layers import Activation\n",
    "# from keras.layers import Activation\n",
    "np.random.seed(seed)\n",
    "act1 = layers.LeakyReLU(alpha=0.1)\n",
    "model = Sequential()\n",
    "model.add(Dense(h[0], input_dim = w*len_data[1], kernel_initializer= 'lecun_uniform', bias_initializer = 'lecun_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('tanh'))\n",
    "act2=layers.LeakyReLU(alpha=0.1)\n",
    "model.add(Dense(h[1], kernel_initializer= 'lecun_uniform', bias_initializer = 'lecun_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(act2)\n",
    "act3=layers.LeakyReLU(alpha=0.1)\n",
    "model.add(Dense(h[2], kernel_initializer= 'lecun_uniform', bias_initializer = 'lecun_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(act3)\n",
    "model.add(Dense(len_data[1], kernel_initializer= 'lecun_uniform', bias_initializer = 'lecun_uniform'))\n",
    "#############################################\n",
    "import os\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "    features = {\"X\": tf.FixedLenFeature((w*257), tf.float32),\n",
    "              \"Y\": tf.FixedLenFeature((257), tf.float32)}\n",
    "    parsed_features = tf.parse_single_example(example_proto, features)\n",
    "    return parsed_features[\"X\"], parsed_features[\"Y\"]\n",
    "\n",
    "tfrecord_path = os.path.normpath(os.path.join(Data_path,tfrecord_folder_parent,tfrecord_folder))\n",
    "\n",
    "subset = 'filenum'\n",
    "files = tf.matching_files(os.path.normpath(os.path.join(tfrecord_path, '%s*' % subset)))\n",
    "shards = tf.data.Dataset.from_tensor_slices(files)\n",
    "shards = shards.shuffle(tf.cast(tf.shape(files)[0], tf.int64))\n",
    "shards = shards.repeat()\n",
    "dataset = shards.interleave(tf.data.TFRecordDataset, cycle_length=20)\n",
    "#it reads from 6 files simoltaneously\n",
    "dataset = dataset.shuffle(buffer_size=buffersize)\n",
    "parser = _parse_function\n",
    "dataset = dataset.apply(\n",
    "   tf.data.experimental.map_and_batch(\n",
    "       map_func=parser,\n",
    "       batch_size=batch_size))\n",
    "#        ,num_parallel_calls=config.NUM_DATA_WORKERS))\n",
    "dataset = dataset.prefetch(batch_size)\n",
    "\n",
    "\n",
    "\n",
    "tfrecord_path_val = os.path.normpath(os.path.join(Data_path,tfrecord_folder_parent,tfrecord_val_folder))\n",
    "sorted_names_val = natsorted(os.listdir(tfrecord_path_val))\n",
    "trainfilenames_val = []\n",
    "for i in sorted_names_val:\n",
    "    trainfilenames_val.append(os.path.normpath(os.path.join(tfrecord_path_val,i)))\n",
    "\n",
    "dataset_val = tf.data.TFRecordDataset(trainfilenames_val)\n",
    "dataset_val = dataset_val.map(_parse_function)  # Parse the record into tensors.\n",
    "dataset_val = dataset_val.repeat()  # Repeat the input indefinitely.\n",
    "dataset_val = dataset_val.batch(128)\n",
    "\n",
    "\n",
    "epochs_num = 50\n",
    "\n",
    "files = glob.glob(os.path.join(result_path,\"checkpoints\",ckpt_folder,'*'))\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "if not os.path.exists(os.path.join(result_path,\"checkpoints\",ckpt_folder)):\n",
    "    os.makedirs(os.path.join(result_path,\"checkpoints\",ckpt_folder))\n",
    "\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "print(\"initialized\")\n",
    "checkpoint_path = os.path.normpath(os.path.join(result_path,\"checkpoints\",ckpt_folder,\"weights.{epoch:02d}.hdf5\"))\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path, verbose=1, save_weights_only=True)\n",
    "\n",
    "opt = tf.keras.optimizers.Adam()\n",
    "\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    def mel_filter(dft_signal):\n",
    "#         sample_rate = tf.constant(16000, dtype =tf.float32)\n",
    "#         nfilt = tf.constant(40, dtype =tf.int32)\n",
    "#         NFFT = tf.constant(512, dtype =tf.float32)\n",
    "#         low_freq_mel = tf.constant(0, dtype =tf.float32)\n",
    "#         log_10 = tf.log(1 + (sample_rate / 2) / 700)/tf.log(tf.constant(10, dtype =tf.float32))\n",
    "#         high_freq_mel = 2595 * log_10  # Convert Hz to Mel\n",
    "#         mel_points = tf.linspace(low_freq_mel, high_freq_mel, nfilt+2)  # Equally spaced in Mel scale\n",
    "#         hz_points = 700 * (10**(mel_points / 2595) - 1)  # Convert Mel to Hz\n",
    "#         binf = tf.floor((NFFT + 1) * hz_points / sample_rate)\n",
    "\n",
    "        sample_rate = 16000\n",
    "        nfilt = 40\n",
    "        NFFT = 512\n",
    "        low_freq_mel = 0\n",
    "        # high_freq_mel = 2595 * np.log(1 + (sample_rate / 2) / 700)/np.log(10)\n",
    "        high_freq_mel = (2595 * np.log10(1 + (sample_rate / 2) / 700))  # Convert Hz to Mel\n",
    "        mel_points = np.linspace(low_freq_mel, high_freq_mel, nfilt + 2)  # Equally spaced in Mel scale\n",
    "        hz_points = (700 * (10**(mel_points / 2595) - 1))  # Convert Mel to Hz\n",
    "        binf = (np.floor((NFFT + 1) * hz_points / sample_rate)).astype(np.int32)\n",
    "        \n",
    "        def trans_1(f_m_minus,f_m,m,fbank,binf):\n",
    "#             global fbank\n",
    "            (\"maybe here?\")\n",
    "#             fbank[m-1,f_m_minus].assign((f_m_minus - binf[m - 1]) / (binf[m] - binf[m - 1]))\n",
    "            fbank[m-1,f_m_minus] = (f_m_minus - binf[m - 1]) / (binf[m] - binf[m - 1])\n",
    "            print(\"or here\")\n",
    "            return [f_m_minus+int(1),f_m,m,fbank,binf]\n",
    "\n",
    "        def trans_2(f_m,f_m_plus,m,fbank,binf):\n",
    "#             global fbank\n",
    "#             fbank[m-1,f_m].assign((binf[m + 1] - f_m) / (binf[m + 1] - binf[m]))\n",
    "            fbank[m-1,f_m] = (binf[m + 1] - f_m) / (binf[m + 1] - binf[m])\n",
    "            return [f_m+int(1),f_m_plus,m,fbank,binf]\n",
    "    \n",
    "        def cond1(f_m_minus,f_m,m,fbank,binf):\n",
    "            print(\"blah?\")\n",
    "            print(f_m_minus)\n",
    "            print(f_m)\n",
    "            return f_m_minus<f_m\n",
    "        \n",
    "        def cond2(f_m,f_m_plus,m,fbank,binf):\n",
    "            return f_m<f_m_plus\n",
    "        \n",
    "#         fbank = tf.Variable(tf.zeros((nfilt, tf.constant(257,dtype=tf.int32))))\n",
    "        fbank = np.zeros((nfilt, 257))\n",
    "        for m in range(1, 41):\n",
    "            f_m_minus = np.int(binf[m - 1])  # left\n",
    "            f_m = np.int(binf[m])           # center\n",
    "            f_m_plus = np.int(binf[m + 1])    # right\n",
    "            print('blah2')\n",
    "            print(f_m_minus)\n",
    "            print(type(f_m))\n",
    "            print(f_m_plus)\n",
    "#             [_,_,_,fbank,_]=tf.while_loop(cond1, trans_1, [f_m_minus,f_m,m,fbank,binf],return_same_structure=True)\n",
    "            print('blah3')\n",
    "#             [_,_,_,fbank,_]=tf.while_loop(cond2, trans_2, [f_m,f_m_plus,m,fbank,binf],return_same_structure=True)\n",
    "            for k in range(f_m_minus, f_m):\n",
    "                fbank[m - 1, k] = (k - binf[m - 1]) / (binf[m] - binf[m - 1])\n",
    "            for k in range(f_m, f_m_plus):\n",
    "                fbank[m - 1, k] = (binf[m + 1] - k) / (binf[m + 1] - binf[m])\n",
    "        filter_banks = K.dot(dft_signal, K.transpose(fbank))\n",
    "        print('here1')\n",
    "        mask = K.cast(tf.equal(filter_banks,0),tf.float32)\n",
    "        filter_banks = filter_banks + (mask*1e-7)\n",
    "        return filter_banks\n",
    "\n",
    "    def trans_corr(k):\n",
    "        global c\n",
    "        c[k] = -0.5*(tf.log(1-tf.contrib.metrics.streaming_pearson_correlation(filtered_true[k],filtered_pred[k])[0]**2))\n",
    "#         c[k] = K.sum(y_pred)\n",
    "        return c\n",
    "\n",
    "    def wrapper_corr(x):\n",
    "        return tf.py_function(trans_corr, [x], tf.float32)\n",
    "    \n",
    "    true = K.pow(tf.constant(10, dtype=tf.float32),y_true)\n",
    "    print(true)\n",
    "    pred = K.pow(tf.constant(10, dtype=tf.float32),y_pred)\n",
    "    filtered_true = mel_filter(true)\n",
    "    print('here2')\n",
    "    filtered_pred = mel_filter(pred)\n",
    "#     c = tf.zeros((tf.constant(40,dtype=tf.float32),1))\n",
    "#     tf.map_fn(wrapper_corr, tf.range(tf.constant(40, dtype =tf.float32)))\n",
    "#     mi = (1/40)*K.sum(c)\n",
    "    mi = K.mean(K.square(filtered_pred - filtered_true), axis=-1)\n",
    "    return mi\n",
    "\n",
    "\n",
    "# class MI\n",
    "# custom_loss = MI.compute_loss\n",
    "model.compile(loss=custom_loss, optimizer=opt)\n",
    "\n",
    "history = model.fit( dataset, steps_per_epoch=steps,epochs=epochs_num,callbacks = [cp_callback], verbose=1,validation_data=dataset_val,validation_steps=val_steps)\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(os.path.normpath(os.path.join(result_path, 'models', \"model_\"+ckpt_folder+\".json\")), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(os.path.normpath(os.path.join(result_path, 'models', \"model_\"+ckpt_folder+\".h5\")))\n",
    "print(\"Saved model to disk\")\n",
    "# h5f.close()   \n",
    "print(datetime.datetime.now())\n",
    "%matplotlib inline\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.savefig(os.path.normpath(os.path.join(result_path,'images',ckpt_folder+'.png')))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OHV4TFroq4hQ"
   },
   "outputs": [],
   "source": [
    "i = tf.constant(0)\n",
    "c = lambda i: tf.less(i, 10)\n",
    "b = lambda i: tf.add(i, 1)\n",
    "r = tf.while_loop(c, b, [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "DZqicgx2q6BD",
    "outputId": "f6acc00a-9b00-4040-e8e0-a0b68315a3a1"
   },
   "outputs": [],
   "source": [
    "    def trans_corr(c,k):\n",
    "#         tf.assign(c[k],-0.5*(tf.log(1-tf.contrib.metrics.streaming_pearson_correlation(filtered_true[k],filtered_pred[k])[0]**2)))\n",
    "#         c = -0.5*(tf.log(1-tf.contrib.metrics.streaming_pearson_correlation(filtered_true,filtered_pred)[0]**2))\n",
    "        c.append(-0.5*(tf.log(1-tf.contrib.metrics.streaming_pearson_correlation(filtered_true[k],filtered_pred[k])[0]**2)))\n",
    "        return c,tf.add(k,1)\n",
    "    \n",
    "#     def cond(k):\n",
    "#         return tf.less(k,40)\n",
    "    cond = lambda c,k: tf.less(k,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vDgwZDp-q6wC"
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "act1 = layers.LeakyReLU(alpha=0.1)\n",
    "model.add(Dense(h[0], input_dim = w*len_data[1], kernel_initializer= 'lecun_uniform', bias_initializer = 'lecun_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('tanh'))\n",
    "act2=layers.LeakyReLU(alpha=0.1)\n",
    "model.add(Dense(h[1], kernel_initializer= 'lecun_uniform', bias_initializer = 'lecun_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(act2)\n",
    "act3=layers.LeakyReLU(alpha=0.1)\n",
    "model.add(Dense(h[2], kernel_initializer= 'lecun_uniform', bias_initializer = 'lecun_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(act3)\n",
    "model.add(Dense(len_data[1], kernel_initializer= 'lecun_uniform', bias_initializer = 'lecun_uniform'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "OKHXVY-lQZXH",
    "outputId": "976c5473-e38d-4e4d-c1d8-9b583bdc0634"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# EDITABLE PARAMETERS\n",
    "# Read the documentation in the script head for more details\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "# length of input\n",
    "input_len = 1000\n",
    "\n",
    "# The window length of the moving average used to generate\n",
    "# the output from the input in the input/output pair used\n",
    "# to train the LSTM\n",
    "# e.g. if tsteps=2 and input=[1, 2, 3, 4, 5],\n",
    "#      then output=[1.5, 2.5, 3.5, 4.5]\n",
    "tsteps = 2\n",
    "\n",
    "# The input sequence length that the LSTM is trained on for each output point\n",
    "lahead = 1\n",
    "\n",
    "# training parameters passed to \"model.fit(...)\"\n",
    "batch_size = 1\n",
    "epochs = 10\n",
    "\n",
    "# ------------\n",
    "# MAIN PROGRAM\n",
    "# ------------\n",
    "\n",
    "print(\"*\" * 33)\n",
    "if lahead >= tsteps:\n",
    "    print(\"STATELESS LSTM WILL ALSO CONVERGE\")\n",
    "else:\n",
    "    print(\"STATELESS LSTM WILL NOT CONVERGE\")\n",
    "print(\"*\" * 33)\n",
    "\n",
    "np.random.seed(1986)\n",
    "\n",
    "print('Generating Data...')\n",
    "\n",
    "\n",
    "def gen_uniform_amp(amp=1, xn=10000):\n",
    "    \"\"\"Generates uniform random data between\n",
    "    -amp and +amp\n",
    "    and of length xn\n",
    "\n",
    "    # Arguments\n",
    "        amp: maximum/minimum range of uniform data\n",
    "        xn: length of series\n",
    "    \"\"\"\n",
    "    data_input = np.random.uniform(-1 * amp, +1 * amp, xn)\n",
    "    data_input = pd.DataFrame(data_input)\n",
    "    return data_input\n",
    "\n",
    "# Since the output is a moving average of the input,\n",
    "# the first few points of output will be NaN\n",
    "# and will be dropped from the generated data\n",
    "# before training the LSTM.\n",
    "# Also, when lahead > 1,\n",
    "# the preprocessing step later of \"rolling window view\"\n",
    "# will also cause some points to be lost.\n",
    "# For aesthetic reasons,\n",
    "# in order to maintain generated data length = input_len after pre-processing,\n",
    "# add a few points to account for the values that will be lost.\n",
    "to_drop = max(tsteps - 1, lahead - 1)\n",
    "data_input = gen_uniform_amp(amp=0.1, xn=input_len + to_drop)\n",
    "\n",
    "# set the target to be a N-point average of the input\n",
    "expected_output = data_input.rolling(window=tsteps, center=False).mean()\n",
    "\n",
    "# when lahead > 1, need to convert the input to \"rolling window view\"\n",
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.repeat.html\n",
    "if lahead > 1:\n",
    "    data_input = np.repeat(data_input.values, repeats=lahead, axis=1)\n",
    "    data_input = pd.DataFrame(data_input)\n",
    "    for i, c in enumerate(data_input.columns):\n",
    "        data_input[c] = data_input[c].shift(i)\n",
    "\n",
    "# drop the nan\n",
    "expected_output = expected_output[to_drop:]\n",
    "data_input = data_input[to_drop:]\n",
    "\n",
    "print('Input shape:', data_input.shape)\n",
    "print('Output shape:', expected_output.shape)\n",
    "print('Input head: ')\n",
    "print(data_input.head())\n",
    "print('Output head: ')\n",
    "print(expected_output.head())\n",
    "print('Input tail: ')\n",
    "print(data_input.tail())\n",
    "print('Output tail: ')\n",
    "print(expected_output.tail())\n",
    "\n",
    "print('Plotting input and expected output')\n",
    "plt.plot(data_input[0][:10], '.')\n",
    "plt.plot(expected_output[0][:10], '-')\n",
    "plt.legend(['Input', 'Expected output'])\n",
    "plt.title('Input')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def create_model(stateful):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(20,\n",
    "              input_shape=(lahead, 1),\n",
    "              batch_size=batch_size,\n",
    "              stateful=stateful))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "print('Creating Stateful Model...')\n",
    "model_stateful = create_model(stateful=True)\n",
    "\n",
    "\n",
    "# split train/test data\n",
    "def split_data(x, y, ratio=0.8):\n",
    "    to_train = int(input_len * ratio)\n",
    "    # tweak to match with batch_size\n",
    "    to_train -= to_train % batch_size\n",
    "\n",
    "    x_train = x[:to_train]\n",
    "    y_train = y[:to_train]\n",
    "    x_test = x[to_train:]\n",
    "    y_test = y[to_train:]\n",
    "\n",
    "    # tweak to match with batch_size\n",
    "    to_drop = x.shape[0] % batch_size\n",
    "    if to_drop > 0:\n",
    "        x_test = x_test[:-1 * to_drop]\n",
    "        y_test = y_test[:-1 * to_drop]\n",
    "\n",
    "    # some reshaping\n",
    "    reshape_3 = lambda x: x.values.reshape((x.shape[0], x.shape[1], 1))\n",
    "    x_train = reshape_3(x_train)\n",
    "    x_test = reshape_3(x_test)\n",
    "\n",
    "    reshape_2 = lambda x: x.values.reshape((x.shape[0], 1))\n",
    "    y_train = reshape_2(y_train)\n",
    "    y_test = reshape_2(y_test)\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = split_data(data_input, expected_output)\n",
    "print('x_train.shape: ', x_train.shape)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('x_test.shape: ', x_test.shape)\n",
    "print('y_test.shape: ', y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2O64BwuYQif8"
   },
   "outputs": [],
   "source": [
    "print('Training')\n",
    "for i in range(epochs):\n",
    "    print('Epoch', i + 1, '/', epochs)\n",
    "    # Note that the last state for sample i in a batch will\n",
    "    # be used as initial state for sample i in the next batch.\n",
    "    # Thus we are simultaneously training on batch_size series with\n",
    "    # lower resolution than the original series contained in data_input.\n",
    "    # Each of these series are offset by one step and can be\n",
    "    # extracted with data_input[i::batch_size].\n",
    "    model_stateful.fit(x_train,\n",
    "                       y_train,\n",
    "                       batch_size=batch_size,\n",
    "                       epochs=1,\n",
    "                       verbose=1,\n",
    "                       validation_data=(x_test, y_test),\n",
    "                       shuffle=False)\n",
    "    model_stateful.reset_states()\n",
    "\n",
    "print('Predicting')\n",
    "predicted_stateful = model_stateful.predict(x_test, batch_size=batch_size)\n",
    "\n",
    "print('Creating Stateless Model...')\n",
    "model_stateless = create_model(stateful=False)\n",
    "\n",
    "print('Training')\n",
    "model_stateless.fit(x_train,\n",
    "                    y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    shuffle=False)\n",
    "\n",
    "print('Predicting')\n",
    "predicted_stateless = model_stateless.predict(x_test, batch_size=batch_size)\n",
    "\n",
    "# ----------------------------\n",
    "\n",
    "print('Plotting Results')\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(y_test)\n",
    "plt.title('Expected')\n",
    "plt.subplot(3, 1, 2)\n",
    "# drop the first \"tsteps-1\" because it is not possible to predict them\n",
    "# since the \"previous\" timesteps to use do not exist\n",
    "plt.plot((y_test - predicted_stateful).flatten()[tsteps - 1:])\n",
    "plt.title('Stateful: Expected - Predicted')\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot((y_test - predicted_stateless).flatten())\n",
    "plt.title('Stateless: Expected - Predicted')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "du16_2_rnn_mi.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "'Python Interactive'",
   "language": "python",
   "name": "d23b60bd-fde3-4bf7-9fcd-4972ac720241"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
